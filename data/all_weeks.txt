
==============================
FILE: 1. class-one.pdf
PAGE: 1
==============================

Machine Learning
Week 1: Class 1

Introduction to Machine Learning

Definitions, Scope, and Applications



==============================
FILE: 1. class-one.pdf
PAGE: 2
==============================

Introduction
Prashanna Rajbhandari

e Machine Learning Researcher
e Mobile Application Engineer
e Full Stack Developer

Current Role

e Co-Founder and CTO
e Synapse Technologies Pvt. Ltd.

Contact

e prashanna.rajbhandari@ismt.edu.np



==============================
FILE: 1. class-one.pdf
PAGE: 3
==============================

Unit Learning Outcomes

By the end of this unit, students will be able to:

LO . sue

Code Learning Outcome Description

LO1 Analyse the theoretical foundation of machine learning to determine how an
intelligent machine works

Lo2 Investigate the most popular and efficient machine learning algorithms used in
industry

103 Develop a machine learning application using an appropriate programming language
or machine learning tool for solving a real-world problem

Lo4 Evaluate the outcome or result of the application to determine the effectiveness of
the learning algorithm used




==============================
FILE: 1. class-one.pdf
PAGE: 4
==============================

What Is Machine Learning?

Machine Learning is a subfield of Artificial Intelligence that focuses on building systems that
can:

e Learn from historical data
e Identify patterns and relationships
¢ Improve performance over time

e Make predictions or decisions without explicit programming

A machine learning system adapts its internal parameters based on experience.



==============================
FILE: 1. class-one.pdf
PAGE: 5
==============================

Machine Learning vs Traditional Programming

Traditional Programming

Machine Learning

Explicit rules written by humans

Rules inferred automatically

Logic-driven

Data-driven

Static behavior

Adaptive behavior

Difficult to scale

Improves with more data




==============================
FILE: 1. class-one.pdf
PAGE: 6
==============================

Why Machine Learning Matters
Machine learning has become essential due to:

e Massive growth of digital data

¢ Availability of low-cost computing power

e Need for intelligent automation

e Demand for predictive and personalized systems

Many problems today are too complex to be solved using fixed rules.



==============================
FILE: 1. class-one.pdf
PAGE: 7
==============================

Scope of Machine Learning
Machine learning is applicable across a wide range of industries:

¢ Healthcare: diagnosis, medical imaging, drug discovery

e Finance: fraud detection, credit scoring, algorithmic trading
e Education: personalized learning, grading automation

¢ Transportation: self-driving vehicles, traffic prediction

e Law and Governance: document analysis, legal research

Agriculture: crop yield prediction, disease detection



==============================
FILE: 1. class-one.pdf
PAGE: 8
==============================

Real-World Applications
Examples of machine learning systems in everyday life:

e Recommendation engines (Netflix, YouTube, Spotify)
e¢ Spam and phishing detection

e Face recognition and biometric authentication

¢ Voice recognition and language translation

¢ Chatbots and virtual assistants



==============================
FILE: 1. class-one.pdf
PAGE: 9
==============================

Types of Machine Learning (Overview)
Supervised Learning
e Uses labeled data

e Tasks include classification and regression
e Example: email spam detection

Unsupervised Learning

e Uses unlabeled data
e Finds hidden patterns or structures

e Example: customer segmentation
Reinforcement Learning

e Learns through rewards and penalties
e¢ Used in control and decision-making systems
¢ Example: game-playing agents, robotics



==============================
FILE: 1. class-one.pdf
PAGE: 10
==============================

Common Machine Learning Tasks

¢ Classification

e Regression

¢ Clustering

e Anomaly Detection
e Recommendation

Each task requires different algorithms and evaluation techniques.



==============================
FILE: 1. class-one.pdf
PAGE: 11
==============================

Machine Learning Workflow
A typical machine learning pipeline includes:

. Problem Definition

. Data Collection

. Data Cleaning and Preprocessing
. Feature Engineering

. Model Selection and Training

. Model Evaluation

N AO wm BP WD

. Deployment and Monitoring

This workflow will be followed throughout the unit.



==============================
FILE: 1. class-one.pdf
PAGE: 12
==============================

Tools and Languages Used in Machine Learning
Common tools and technologies include:

e Programming Languages: Python, R

e Libraries: NumPy, Pandas, Scikit-learn

¢ Deep Learning Frameworks: TensorFlow, PyTorch
e Visualization Tools: Matplotlib, Seaborn

e Platforms: Jupyter Notebook, Google Colab



==============================
FILE: 1. class-one.pdf
PAGE: 13
==============================

Class Summary

e Understanding theoretical foundations of ML

¢ Gaining exposure to industry-relevant algorithms
¢ Quick Overview of Types of ML

e Understanding ML Pipeline



==============================
FILE: 1. class-one.pdf
PAGE: 14
==============================

How This Unit Will Be Taught

¢ Concept-focused lectures
e Practical demonstrations
e Hands-on labs and coding exercises



==============================
FILE: 1. class-one.pdf
PAGE: 15
==============================

Next Class

Core Terminologies - Features, Labels, Models, Training vs Testing



==============================
FILE: 2. class-two.pdf
PAGE: 1
==============================

Machine Learning
Week 1: Class 2

Core Terminologies

Features, Labels, Models, Training vs Testing



==============================
FILE: 2. class-two.pdf
PAGE: 2
==============================

Class Objective

e Understand key machine learning terminologies
e Identify features and labels in a dataset
e Explain what a model is

e Distinguish between training and testing phases



==============================
FILE: 2. class-two.pdf
PAGE: 3
==============================

Why Terminology Matters
Machine learning relies heavily on precise terminology.
Understanding core terms helps you:

e Read ML papers and documentation
¢ Communicate effectively with teams
e Design correct ML pipelines

e Avoid common beginner mistakes



==============================
FILE: 2. class-two.pdf
PAGE: 4
==============================

What Is Data in Machine Learning?
In machine learning, data is the foundation.
Data typically consists of:

e Inputs (features)
¢ Outputs (labels)
e Examples (rows or records)

The quality of data directly impacts model performance.



==============================
FILE: 2. class-two.pdf
PAGE: 5
==============================

Features
What Are Features?

Features are individual measurable properties or characteristics of data.

They represent the inputs given to a machine learning model.



==============================
FILE: 2. class-two.pdf
PAGE: 6
==============================

Examples of Features

Problem Features

House price prediction | Area, number of rooms, location

Email spam detection | Word frequency, sender, subject

Student performance | Attendance, assignment scores

Features are usually represented as columns in a dataset.



==============================
FILE: 2. class-two.pdf
PAGE: 7
==============================

Feature Types (High-Level)
Common feature types include:

e Numerical (age, salary, marks)

e Categorical (gender, city, product type)
e Binary (yes/no, true/false)

e Text and image-based features

Feature selection is a critical ML step.



==============================
FILE: 2. class-two.pdf
PAGE: 8
==============================

Labels
What Are Labels?

Labels are the outcomes or target values the model is trying to predict.

They represent the correct answer for supervised learning.



==============================
FILE: 2. class-two.pdf
PAGE: 9
==============================

Examples of Labels

Problem Label

House price prediction | House price

Email classification Spam or Not Spam

Disease detection Disease present or not

Labels are usually the final column in a dataset.



==============================
FILE: 2. class-two.pdf
PAGE: 10
==============================

Features vs Labels

Features

Input variables Output variable

Given to model Predicted by model

Independent Dependent

Multiple per dataset | Usually one

Understanding this distinction is essential.



==============================
FILE: 2. class-two.pdf
PAGE: 11
==============================

What Is a Model?
A machine learning model is a mathematical representation that:

e Learns patterns from data
e Maps features to labels
e Makes predictions on new data

The model contains learned parameters.



==============================
FILE: 2. class-two.pdf
PAGE: 12
==============================

Examples of Models
Common machine learning models include:

e Linear Regression

e Decision Trees

e k-Nearest Neighbors

e Support Vector Machines

e Neural Networks

Different problems require different models.



==============================
FILE: 2. class-two.pdf
PAGE: 13
==============================

Training a Model
What Is Training?

Training is the process where a model:

e Learns from historical data
e Adjusts internal parameters

e Minimizes prediction error

Training uses labeled data.



==============================
FILE: 2. class-two.pdf
PAGE: 14
==============================

Training Data
Training data:

e Is the largest portion of the dataset
e Contains both features and labels
e Is used to teach the model patterns

Typically 70-80% of data is used for training.



==============================
FILE: 2. class-two.pdf
PAGE: 15
==============================

Testing a Model
What Is Testing?

Testing evaluates how well the model performs on unseen data.

The model does not learn during testing.



==============================
FILE: 2. class-two.pdf
PAGE: 16
==============================

Testing Data
Testing data:

e Is kept separate from training data
e Contains known labels
e Measures model accuracy and reliability

Typically 20-30% of data is used for testing.



==============================
FILE: 2. class-two.pdf
PAGE: 17
==============================

Training vs Testing

Training Testing

Model learns Model evaluates

Uses majority of data | Uses held-out data

Adjusts parameters | No parameter updates

Risk of overfitting | Checks generalization




==============================
FILE: 2. class-two.pdf
PAGE: 18
==============================

Why Split Data?
Data is split to:

e Prevent memorization

e Measure real-world performance
e Detect overfitting

e Ensure fairness in evaluation

Without splitting, results are misleading.



==============================
FILE: 2. class-two.pdf
PAGE: 19
==============================

Overfitting (Concept Preview)
Overfitting occurs when:

e Model performs very well on training data
e Model performs poorly on testing data

This will be covered in detail in later classes.



==============================
FILE: 2. class-two.pdf
PAGE: 20
==============================

Class Summary

e Features are input variables

e Labels are target outputs

e Models learn patterns from data
e Training teaches the model

e Testing evaluates performance



==============================
FILE: 2. class-two.pdf
PAGE: 21
==============================

Next Class

Types of Learning Problems - Classification



==============================
FILE: 3. class-three.pdf
PAGE: 1
==============================

Machine Learning
Week 1: Class 3

Types of Learning Problems

Classification



==============================
FILE: 3. class-three.pdf
PAGE: 2
==============================

Class Objective
By the end of this class, students will be able to:

e Understand what a classification problem is
e Identify classification tasks in real-world scenarios
e Distinguish classification from regression

e¢ Recognize common classification algorithms



==============================
FILE: 3. class-three.pdf
PAGE: 3
==============================

Types of Learning Problems
In supervised machine learning, problems are commonly divided into:

¢ Classification
e Regression

This class focuses on classification.



==============================
FILE: 3. class-three.pdf
PAGE: 4
==============================

What Is Classification?

Classification is a supervised learning problem where:

e The output (label) belongs to a finite set of categories
e The model predicts a class rather than a number

The goal is to assign the correct label to new data.



==============================
FILE: 3. class-three.pdf
PAGE: 5
==============================

Key Characteristics of Classification
Classification problems typically involve:

e Labeled training data
e Discrete output values
e Decision boundaries between classes

Examples include binary and multi-class problems.



==============================
FILE: 3. class-three.pdf
PAGE: 6
==============================

Binary Classification
Binary classification involves two possible classes.
Examples:

e Spam or Not Spam

e Fraud or Not Fraud

¢ Pass or Fail

e Disease or No Disease

This is the simplest form of classification.



==============================
FILE: 3. class-three.pdf
PAGE: 7
==============================

Multi-Class Classification
Multi-class classification involves more than two classes.
Examples:

e¢ Handwritten digit recognition (0-9)
e News article categorization
e Student grade classification (A, B, C, D, F)

Each data point belongs to exactly one class.



==============================
FILE: 3. class-three.pdf
PAGE: 8
==============================

Examples of Classification Problems

Problem Domain Classification Task

Email systems Spam detection

Finance Loan approval

Healthcare Disease diagnosis

Education Student performance category
Security Face recognition




==============================
FILE: 3. class-three.pdf
PAGE: 9
==============================

Classification vs Regression

Classification Regression

Predicts categories Predicts continuous values

Discrete output Numerical output

Example: Spam/Not Spam | Example: House price

Understanding this distinction is critical.



==============================
FILE: 3. class-three.pdf
PAGE: 10
==============================

Common Classification Algorithms
Some widely used classification algorithms:

e Logistic Regression

e k-Nearest Neighbors (k-NN)

e Decision Trees

e Support Vector Machines (SVM)
e Naive Bayes

e¢ Neural Networks

Algorithm choice depends on the problem and data.



==============================
FILE: 3. class-three.pdf
PAGE: 11
==============================

Logistic Regression
What It Is

e Asimple and widely used classification algorithm
e Used for predicting two possible outcomes

How It Works

¢ Combines input features using weights
¢ Outputs a probability between 0 and 1
e Uses a threshold to decide the class

When to Use

e When the outcome has two classes
e Predicting Pass / Fail
e¢ Email Spam / Not Spam



==============================
FILE: 3. class-three.pdf
PAGE: 12
==============================

k-Nearest Neighbors (k-NN)
What It Is

¢ Aclassification algorithm based on similarity
e Does not build a model in advance

How It Works

¢ Looks at the k closest data points

e Assigns the most common class among them
When to Use

e¢ When the dataset is small

e¢ When similar data points usually belong to the same class
¢ Classifying students based on marks and attendance

e Recommending items based on similar users



==============================
FILE: 3. class-three.pdf
PAGE: 13
==============================

Decision Trees
What It Is

e Atree-shaped model that uses decision rules

e Very easy to understand and explain
How It Works

e Splits data using questions
e Each split reduces uncertainty
e Final decision is made at leaf nodes

When to Use

e When decisions must be clearly explained

e When rules matter more than accuracy

e Loan approval based on income and credit score
e Student eligibility decisions



==============================
FILE: 3. class-three.pdf
PAGE: 14
==============================

Support Vector Machines (SVM)
What It Is

¢ A powerful classification algorithm
e Works well with complex data

How It Works

e Finds the best boundary separating classes

e Maximizes the distance between classes
When to Use

e When data has many features

e When classes are clearly separable
e Face recognition

¢ Handwritten character recognition



==============================
FILE: 3. class-three.pdf
PAGE: 15
==============================

Naive Bayes
What It Is

e A probability-based classification algorithm

e Assumes features are independent
How It Works

e¢ Calculates probability for each class
e Selects the class with the highest probability

When to Use

e When dealing with text data
e¢ When fast training is needed
¢ Email spam filtering

e¢ Sentiment analysis of reviews



==============================
FILE: 3. class-three.pdf
PAGE: 16
==============================

Neural Networks
What It Is

e A flexible and powerful learning model

e Inspired by how the human brain works
How It Works

¢ Data flows through multiple layers

e Learns complex patterns automatically
When to Use

e When the problem is complex
e¢ When large amounts of data are available
¢ Image recognition

e Voice assistants and chatbots



==============================
FILE: 3. class-three.pdf
PAGE: 17
==============================

Algorithm Selection (Beginner View)

Problem Type Suggested Algorithm

Simple binary decision Logistic Regression
Small, similarity-based data | k-NN

Rule-based decisions Decision Trees
High-dimensional data SVM
Text classification Naive Bayes

Images or speech Neural Networks




==============================
FILE: 3. class-three.pdf
PAGE: 18
==============================

Key Takeaway

e Start with simple models

¢ Choose algorithms based on:
°o Data size
oc Problem complexity
o Need for explanation

More advanced models will be explored later.



==============================
FILE: 3. class-three.pdf
PAGE: 19
==============================

Classification Output
Classification models may output:

e A predicted class label
e¢ Probabilities for each class

Example:

e Spam (0.92)
e¢ Not Spam (0.08)



==============================
FILE: 3. class-three.pdf
PAGE: 20
==============================

Decision Boundary (Conceptual)
A decision boundary separates different classes.

e Learned during training
e Depends on features and model
¢ Can be linear or non-linear

We will visualize decision boundaries in later labs.



==============================
FILE: 3. class-three.pdf
PAGE: 21
==============================

Evaluating Classification Models

Common evaluation metrics include:

Accuracy

Precision

Recall

Fi-score

Confusion Matrix

Proper evaluation ensures model reliability.



==============================
FILE: 3. class-three.pdf
PAGE: 22
==============================

Classification Workflow

1. Define the classification problem
2. Collect and label data

3. Select features

4. Split data (training/testing)

5. Train classification model

6. Evaluate performance

This mirrors the general ML pipeline.



==============================
FILE: 3. class-three.pdf
PAGE: 23
==============================

Real-World Considerations
When solving classification problems:

e Data may be imbalanced
e Misclassification costs vary
e Accuracy alone may be misleading

These issues will be addressed in future classes.



==============================
FILE: 3. class-three.pdf
PAGE: 24
==============================

Class Summary

¢ Classification predicts categorical outputs
¢ Can be binary or multi-class

e Uses labeled data

e Requires appropriate evaluation metrics

¢ Widely used in real-world applications



==============================
FILE: 3. class-three.pdf
PAGE: 25
==============================

Next Class

Types of Learning Problems - Regression



==============================
FILE: 4. class-four.pdf
PAGE: 1
==============================

Machine Learning
Week 1: Class 4

Types of Learning Problems

Regression



==============================
FILE: 4. class-four.pdf
PAGE: 2
==============================

Class Objective
By the end of this class, students will be able to:

e¢ Understand what a regression problem is
e Identify real-world regression tasks
e Distinguish regression from classification

¢ Recognize common regression algorithms



==============================
FILE: 4. class-four.pdf
PAGE: 3
==============================

Types of Learning Problems
In supervised machine learning, common problem types include:

¢ Classification
e Regression

This class focuses on regression.



==============================
FILE: 4. class-four.pdf
PAGE: 4
==============================

What Is Regression?
Regression is a supervised learning problem where:

e The output (label) is a continuous numerical value
e The model predicts quantities rather than categories

The goal is to estimate an accurate numeric outcome.



==============================
FILE: 4. class-four.pdf
PAGE: 5
==============================

Key Characteristics of Regression
Regression problems typically involve:

e Labeled training data
¢ Continuous output values
e Error measured as distance from true value

Examples include price, score, or demand prediction.



==============================
FILE: 4. class-four.pdf
PAGE: 6
==============================

Examples of Regression Problems

Problem Domain Regression Task

Real estate House price prediction
Finance Stock price forecasting
Education Student score prediction
Healthcare Blood pressure estimation
Weather Temperature prediction




==============================
FILE: 4. class-four.pdf
PAGE: 7
==============================

Regression vs Classification

Regression Classification

Predicts numerical values | Predicts categories

Continuous output Discrete output

Example: House price Example: Spam detection

Understanding this distinction is essential.



==============================
FILE: 4. class-four.pdf
PAGE: 8
==============================

Simple vs Multiple Regression

Simple Regression

e Uses one input feature
e Example: Price vs area

Multiple Regression

e Uses multiple input features
e Example: Price vs area, location, rooms



==============================
FILE: 4. class-four.pdf
PAGE: 9
==============================

Linear Regression (Concept Overview)
Linear regression models:

e A linear relationship between inputs and output
e A straight-line fit to the data

It is one of the most widely used regression techniques.



==============================
FILE: 4. class-four.pdf
PAGE: 10
==============================

Non-Linear Regression (Concept Overview)
Non-linear regression models:

e Complex relationships
e Curved or irregular patterns
e Real-world data behavior

Often handled using advanced models.



==============================
FILE: 4. class-four.pdf
PAGE: 11
==============================

Common Regression Algorithms

Popular regression algorithms include:

Linear Regression
Polynomial Regression
Ridge and Lasso Regression
Decision Tree Regression
Random Forest Regression
Neural Networks



==============================
FILE: 4. class-four.pdf
PAGE: 12
==============================

Linear Regression
What It Is

e The simplest and most commonly used regression algorithm

e Models a straight-line relationship between inputs and output
How It Works

e Fits a straight line that best represents the data

e Minimizes the difference between predicted and actual values
When to Use

e When the relationship is roughly linear

e When you want a simple and interpretable model
e Predicting house price based on area

e Estimating salary based on years of experience



==============================
FILE: 4. class-four.pdf
PAGE: 13
==============================

Polynomial Regression
What It Is

e An extension of linear regression

e Can model curved relationships
How It Works

e¢ Transforms input features into polynomial terms
e Fits a curved line instead of a straight line

When to Use

e When data shows a non-linear trend
e When linear regression underfits
e Predicting growth trends over time

¢ Modeling exam score vs study hours where improvement slows



==============================
FILE: 4. class-four.pdf
PAGE: 14
==============================

Ridge and Lasso Regression
What They Are

e Regularized versions of linear regression

e Help prevent overfitting
How They Work

e Penalize large coefficients

e Reduce model complexity
When to Use

e When there are many input features

e When features are correlated

e Predicting house price using many property features
¢ Financial prediction models with many variables



==============================
FILE: 4. class-four.pdf
PAGE: 15
==============================

Decision Tree Regression
What It Is

e A tree-based regression model

e Uses rules to make predictions
How It Works

e Splits data based on feature conditions
e¢ Outputs average value at leaf nodes

When to Use

e When relationships are non-linear

e When explanation is important

e Predicting insurance cost based on age and risk factors
e Estimating sales based on season and region



==============================
FILE: 4. class-four.pdf
PAGE: 16
==============================

Random Forest Regression
What It Is

e An ensemble of multiple decision trees

e Produces more stable predictions
How It Works

¢ Builds many trees on random subsets of data

e Averages predictions from all trees
When to Use

e When accuracy is more important than simplicity

e¢ When data is noisy

e Predicting house prices with many influencing factors
e Demand forecasting for products



==============================
FILE: 4. class-four.pdf
PAGE: 17
==============================

Neural Networks (Regression)
What It Is

e A powerful and flexible regression model

e Can learn very complex patterns
How It Works

e¢ Passes data through multiple layers of neurons

e Adjusts weights to minimize prediction error
When to Use

e When data is large and complex
¢ When simpler models fail

e Predicting energy consumption
e Stock price trend estimation



==============================
FILE: 4. class-four.pdf
PAGE: 18
==============================

Regression Algorithm Comparison (Beginner View)

Problem Type Suggested Algorithm

Simple linear relationship Linear Regression
Curved trend Polynomial Regression
Many correlated features Ridge / Lasso

Rule-based numeric prediction | Decision Tree Regression

High accuracy needed Random Forest

Complex patterns, large data Neural Networks




==============================
FILE: 4. class-four.pdf
PAGE: 19
==============================

Key Takeaway

e Start with simple regression models
¢ Increase complexity only when needed
e Algorithm choice depends on:

°o Data size

co Feature complexity

© Interpretability needs

Understanding regression algorithms builds a strong ML foundation.



==============================
FILE: 4. class-four.pdf
PAGE: 20
==============================

Regression Output
Regression models produce:

e Asingle numerical prediction
e Continuous-valued output

Example:

e Predicted house price: 7,500,000



==============================
FILE: 4. class-four.pdf
PAGE: 21
==============================

Evaluating Regression Models
Common evaluation metrics include:

e Mean Absolute Error (MAE)

e Mean Squared Error (MSE)

e Root Mean Squared Error (RMSE)
e R-squared (R?)

Lower error indicates better performance.



==============================
FILE: 4. class-four.pdf
PAGE: 22
==============================

Regression Workflow

1. Define the regression problem
2. Collect and prepare data

3. Select relevant features

4. Split data (training/testing)

5. Train regression model

6. Evaluate predictions

This workflow mirrors general ML practice.



==============================
FILE: 4. class-four.pdf
PAGE: 23
==============================

Real-World Considerations
When working with regression problems:

¢ Outliers can distort results
e Data scaling may be required
e Linear assumptions may not hold

These will be addressed in later sessions.



==============================
FILE: 4. class-four.pdf
PAGE: 24
==============================

Class Summary

e Regression predicts continuous values
e Uses labeled data

¢ Can be linear or non-linear

e Requires numerical evaluation metrics

¢ Widely used in prediction tasks



==============================
FILE: 4. class-four.pdf
PAGE: 25
==============================

Next Class

Types of Learning Problems - Clustering & Optimization



==============================
FILE: 5. class-five.pdf
PAGE: 1
==============================

Machine Learning
Week 1- Class 5

Types of Learning Problems

Clustering & Optimization



==============================
FILE: 5. class-five.pdf
PAGE: 2
==============================

Class Objective
By the end of this class, students will be able to:

e Understand unsupervised learning concepts
e Explain clustering and its use cases

e Understand optimization in machine learning
e Identify real-world examples of both



==============================
FILE: 5. class-five.pdf
PAGE: 3
==============================

Types of Learning Problems (Recap)
So far, we have covered:

¢ Classification
e Regression

This class introduces unsupervised learning and optimization.



==============================
FILE: 5. class-five.pdf
PAGE: 4
==============================

What Is Unsupervised Learning?
In unsupervised learning:

¢ Data is unlabeled
e The system discovers patterns automatically
e No predefined output is provided

Clustering is one of the most common unsupervised learning tasks.



==============================
FILE: 5. class-five.pdf
PAGE: 5
==============================

What Is Clustering?
Clustering is a learning problem where:

e Data points are grouped based on similarity
e Similar items are placed in the same group (cluster)
e No labels are given beforehand

The goal is to uncover hidden structure in data.



==============================
FILE: 5. class-five.pdf
PAGE: 6
==============================

Key Characteristics of Clustering
Clustering problems typically involve:

e Unlabeled data
e Similarity or distance measures
e Grouping based on patterns

Clusters are formed based on feature similarity.



==============================
FILE: 5. class-five.pdf
PAGE: 7
==============================

Examples of Clustering Problems

Domain Clustering Use Case

Marketing Customer segmentation
Education Student behavior grouping
Healthcare Patient risk profiling
Finance Spending pattern analysis
Image processing | Image segmentation




==============================
FILE: 5. class-five.pdf
PAGE: 8
==============================

Common Clustering Algorithms
Popular clustering algorithms include:

e K-Means

¢ Hierarchical Clustering
¢ DBSCAN

¢ Mean Shift

Each algorithm defines similarity differently.



==============================
FILE: 5. class-five.pdf
PAGE: 9
==============================

K-Means Clustering
K-Means is a centroid-based clustering algorithm.
Key ideas:

e Each cluster has a center (centroid)
e Data points belong to the nearest centroid

¢ Centroids are updated iteratively



==============================
FILE: 5. class-five.pdf
PAGE: 10
==============================

Characteristics of K-Means

e Fast and efficient for large datasets
¢ Works well with spherical clusters
e Sensitive to:

© Initial centroid selection

© Outliers

o Feature scaling

Requires the value of K beforehand.



==============================
FILE: 5. class-five.pdf
PAGE: 11
==============================

Hierarchical Clustering
Hierarchical clustering builds clusters in a tree-like structure.

Produces a dendrogram to visualize clustering.



==============================
FILE: 5. class-five.pdf
PAGE: 12
==============================

Characteristics of Hierarchical Clustering

e Easy to visualize and interpret
e Does not require K upfront
e Computationally expensive

e Not suitable for very large datasets

Good for exploratory data analysis.



==============================
FILE: 5. class-five.pdf
PAGE: 13
==============================

DBSCAN (Density-Based Clustering)
DBSCAN groups data based on density.
Key concepts:

e Dense regions form clusters
e Sparse regions are considered noise

¢ Clusters can have arbitrary shapes



==============================
FILE: 5. class-five.pdf
PAGE: 14
==============================

Characteristics of DBSCAN

e¢ Automatically detects number of clusters
e Handles noise and outliers well
e Works with irregular cluster shapes

e Struggles with varying densities

Widely used in spatial and anomaly detection tasks.



==============================
FILE: 5. class-five.pdf
PAGE: 15
==============================

Mean Shift Clustering
Mean Shift is a centroid-based but non-parametric algorithm.
It works by:

e Placing a window around data points
e Shifting the window toward dense regions

e Converging to cluster centers automatically



==============================
FILE: 5. class-five.pdf
PAGE: 16
==============================

Characteristics of Mean Shift

¢ No need to specify number of clusters
e Can find arbitrarily shaped clusters

e Computationally expensive

e Sensitive to bandwidth selection

Often used in image segmentation.



==============================
FILE: 5. class-five.pdf
PAGE: 17
==============================

Summary of Clustering Algorithms

Algorithm Needs K? Handles Noise Shape
K-Means Yes No Spherical
Hierarchical | No No Tree-based
DBSCAN No Yes Arbitrary
Mean Shift | No Limited Arbitrary




==============================
FILE: 5. class-five.pdf
PAGE: 18
==============================

Challenges in Clustering
Common challenges include:

e Choosing the right number of clusters
e Handling noise and outliers

e Scaling features

e Interpreting cluster meaning

Clustering results often require domain knowledge.



==============================
FILE: 5. class-five.pdf
PAGE: 19
==============================

What Is Optimization?
Optimization in machine learning refers to:

e Finding the best model parameters
e Minimizing error or loss
e Improving model performance

Optimization drives learning in most ML algorithms.



==============================
FILE: 5. class-five.pdf
PAGE: 20
==============================

Optimization in Simple Terms

Optimization answers the question:

"How do we make the model as accurate as possible? "
This is done by:

e Defining a loss function
e Minimizing that loss



==============================
FILE: 5. class-five.pdf
PAGE: 21
==============================

Loss Function (Concept Overview)
A loss function measures:

¢ How wrong a model’s prediction is

e Difference between predicted and actual values

Lower loss means better performance.



==============================
FILE: 5. class-five.pdf
PAGE: 22
==============================

Optimization Example
Example: House price prediction

e Model predicts price

e Compare prediction with actual price
¢ Calculate error

e Adjust model parameters

e Repeat until error is minimized

This process is optimization.



==============================
FILE: 5. class-five.pdf
PAGE: 23
==============================

Common Optimization Techniques
Some commonly used techniques include:

e Gradient Descent
e Stochastic Gradient Descent (SGD)
e Mini-batch Gradient Descent

These methods update parameters iteratively.



==============================
FILE: 5. class-five.pdf
PAGE: 24
==============================

Optimization vs Clustering

Clustering Optimization

Groups similar data | Improves model performance

Unsupervised task | Used across ML tasks

No labels required | Uses loss functions

Pattern discovery Error minimization

Both play critical roles in ML systems.



==============================
FILE: 5. class-five.pdf
PAGE: 25
==============================

Real-World Importance
Clustering and optimization are used in:

e Recommendation systems

¢ Customer analytics

e Image and speech recognition
e Training deep learning models

They form the backbone of modern ML pipelines.



==============================
FILE: 5. class-five.pdf
PAGE: 26
==============================

Class Summary

¢ Clustering groups unlabeled data

e It is a key unsupervised learning task

¢ Optimization improves model accuracy
e Loss functions guide optimization

e Both are essential in real-world ML



==============================
FILE: 5. class-five.pdf
PAGE: 27
==============================

Next Class

Supervised Learning - Concepts and Workflow



==============================
FILE: 1. class-six.pdf
PAGE: 1
==============================

Machine Learning

Python Basics - Foundations for Machine Learning

Unit I - Programming Fundamentals



==============================
FILE: 1. class-six.pdf
PAGE: 2
==============================

Class Objective
By the end of this class, students will be able to:

e Write and run basic Python programs

e Understand variables and data types

e Use arithmetic, comparison, and logical operators

¢ Perform type conversion and string operations

e¢ Use common built-in Python functions

e Practice coding using Jupyter Notebook / Google Colab



==============================
FILE: 1. class-six.pdf
PAGE: 3
==============================

Why Python for Machine Learning?

e Simple and readable syntax
e Beginner-friendly language

e Large ecosystem of libraries
Popular Python Libraries for DS

e NumPy

e Pandas

e Matplotlib / Seaborn
¢ Scikit-learn

e TensorFlow / PyTorch



==============================
FILE: 1. class-six.pdf
PAGE: 4
==============================

Python Basics: First Program

A Python program can be written in just one line:




==============================
FILE: 1. class-six.pdf
PAGE: 5
==============================

Checking Your Python Version

sys
print( , sys.version)

Why this is important:
e Libraries may require specific versions
e Helps debug compatibility issues



==============================
FILE: 1. class-six.pdf
PAGE: 6
==============================

Variables in Python
Variables store data values:

name = "Sita"
age = 21

gpa = 3.75

is_ student = True

Python automatically assigns data types

No need for explicit type declaration



==============================
FILE: 1. class-six.pdf
PAGE: 7
==============================

Variable Naming Rules

Start with a letter or underscore
Case-sensitive (age # Age)
Cannot use Python keywords
Use descriptive names

~ Invalid:

2name = "Ram"
class = "DS"

Valid:

student_name = "Ram"
total_marks = 85



==============================
FILE: 1. class-six.pdf
PAGE: 8
==============================

Python Data Types (Primitive)

Data Type Example

int 42

float 3.14

str "Hello"
bool True, False
NoneType None




==============================
FILE: 1. class-six.pdf
PAGE: 9
==============================

Checking Data Types

age = 25
print(type(age))

Output

<class >



==============================
FILE: 1. class-six.pdf
PAGE: 10
==============================

Python Arithmetic Operators

Addition (+)
Subtraction (-)
Multiplication (*)
Division (/)

Floor Division (//)
Modulus (%)
Exponentiation (**)



==============================
FILE: 1. class-six.pdf
PAGE: 11
==============================

Arithmetic Operator Examples

print(a + b)

print(a / b)
print(a // b)
print(a % b)
print(a ** b)




==============================
FILE: 1. class-six.pdf
PAGE: 12
==============================

Comparison Operators

Used to compare values:
e == equal

e '= not equal
@<,>,<=,>=

x
y

print(x == y)
print(x < y)




==============================
FILE: 1. class-six.pdf
PAGE: 13
==============================

Logical Operators

Used to combine conditions:
e and

eor

e not

print((x != y) (x < y))



==============================
FILE: 1. class-six.pdf
PAGE: 14
==============================

Type Conversion
Convert data from one type to another:

age = int("25")
price = float("19.99")
score = str(95)

Why needed:
e User input is usually string
e ML models need numeric data



==============================
FILE: 1. class-six.pdf
PAGE: 15
==============================

String Operations
Concatenation

first_name =
last_name =

full_name = first_name + + last_name
print(full_name)

f-Strings (Recommended)

message =
print(message)



==============================
FILE: 1. class-six.pdf
PAGE: 16
==============================

String Indexing

word =

print(word[@])
print(word[-1])

. Index starts at @
. Negative index counts from end




==============================
FILE: 1. class-six.pdf
PAGE: 17
==============================

Built-in Python Functions

Common functions:
e print()

e type()

e len()

e abs()

e round()

e max()

e min()

print(type(42))
print(len( ))

print(abs(-1@))
print(round( ))




==============================
FILE: 1. class-six.pdf
PAGE: 18
==============================

Hands-On Exercise #1

Tasks:

e Print a welcome message

e Create variables (name, age, GPA)
e Perform arithmetic operations

e Practice type conversion



==============================
FILE: 1. class-six.pdf
PAGE: 19
==============================

Hands-On Exercise #2

Tasks:

e Write boolean expressions
e Perform string operations
e Use f-strings

e Apply built-in functions



==============================
FILE: 1. class-six.pdf
PAGE: 20
==============================

Common Beginner Mistakes

- Indentation errors
_ Mixing data types

"Age: "+ 21 # Error

_ Using keywords as variables
- Case sensitivity issues



==============================
FILE: 1. class-six.pdf
PAGE: 21
==============================

Knowledge Check

List three stages of Machine Learning workflow
Name three Python libraries for Machine Learning

What is the data type of 3.14?
What does // operator do?
Convert "100" into an integer




==============================
FILE: 1. class-six.pdf
PAGE: 22
==============================

Next Class
Python Programming Basics & Operators

(Control Flow, Conditions, Loops)



==============================
FILE: 2. class-seven.pdf
PAGE: 1
==============================

Machine Learning

Python Programming Basics & Operators

Control Flow - Conditions - Loops



==============================
FILE: 2. class-seven.pdf
PAGE: 2
==============================

Class Objective
By the end of this class, students will be able to:

e Understand how programs make decisions
Use conditional statements (Fj, Bikta, BES)
¢ Implement repetition using and loops

* Control loop execution using and

¢ Combine conditions and loops to solve real problems



==============================
FILE: 2. class-seven.pdf
PAGE: 3
==============================

Why Control Flow Matters
Without control flow, programs:

¢ Run line by line only once
¢ Cannot make decisions
e Cannot repeat tasks

Control flow allows programs to:

¢ Choose different paths
e React to data
¢ Automate repetitive work

This is fundamental to machine learning pipelines.



==============================
FILE: 2. class-seven.pdf
PAGE: 4
==============================

Control Flow in Python
Python provides:

e Conditional statements — decision making
e Loops > repetition
e Loop controls — precise execution control

These structures define program logic.



==============================
FILE: 2. class-seven.pdf
PAGE: 5
==============================

Conditional Statements — Overview

Conditional statements execute code only if a condition is true.

Python keywords:

i

De elif

i False



==============================
FILE: 2. class-seven.pdf
PAGE: 6
==============================

if Statement

Syntax:

condition:

Example:




==============================
FILE: 2. class-seven.pdf
PAGE: 7
==============================

if-else Statement

Syntax:

if condition:

else:

Example:

marks =

if marks >=

print("Pass")
else:
print("Fail")




==============================
FILE: 2. class-seven.pdf
PAGE: 8
==============================

if-elif-else Statement
Used when multiple conditions exist.
Example:

score =

if score >=
print("Excellent")
elif score >= :

print("Good")
elif score >=

print("Pass")
else:

print("Fail")

Only one block executes.




==============================
FILE: 2. class-seven.pdf
PAGE: 9
==============================

Conditions & Comparison Operators

|==| Equal

|t=| Not equal

Less than

Greater than

<= Less than or equal

>= Greater than or equal
Example:

y =
print(x < y)




==============================
FILE: 2. class-seven.pdf
PAGE: 10
==============================

Logical Operators in Conditions

Operator Meaning

land] Both conditions true
At least one true

not Reverse condition
Example:

age = 25

age >= 18 age < 60:
print("Working age group")




==============================
FILE: 2. class-seven.pdf
PAGE: 11
==============================

Common Conditional Mistakes

¢ Using Bj instead of

* Forgetting colon

e Incorrect indentation

e Overly complex nested conditions

Tip: keep conditions simple and readable.



==============================
FILE: 2. class-seven.pdf
PAGE: 12
==============================

Loops — Why We Need Them

Loops are used to:

¢ Repeat tasks
e Process lists and datasets
e Train models over multiple iterations

Python supports:

° loop
+ [EEEEY loop



==============================
FILE: 2. class-seven.pdf
PAGE: 13
==============================

for Loop
Used to iterate over sequences.
Syntax:

variable sequence:

Example:

i range(1, 6):
print(i)

Output:

12345



==============================
FILE: 2. class-seven.pdf
PAGE: 14
==============================

for Loop with List

fruits = [

fruit fruits:
print(fruit)

Common in:

e Dataset processing
e Feature iteration



==============================
FILE: 2. class-seven.pdf
PAGE: 15
==============================

while Loop

Repeats as long as condition is true.

Syntax:

condition:

Example:

count =
count <=

print(count)
count +=




==============================
FILE: 2. class-seven.pdf
PAGE: 16
==============================

Avoiding Infinite Loops
Infinite loop occurs when:

¢ Condition never becomes false
Bad example:

True:
print( )

Always:

e Update loop variable

e Ensure exit condition



==============================
FILE: 2. class-seven.pdf
PAGE: 17
==============================

Loop Control Statements
Python provides:

° — exit loop immediately
° — skip current iteration

Used for fine-grained control.



==============================
FILE: 2. class-seven.pdf
PAGE: 18
==============================

break Statement

num range(1, ):

num ==

print(num)

Output:

1234

Stops loop entirely.



==============================
FILE: 2. class-seven.pdf
PAGE: 19
==============================

continue Statement

num range(1,

num % ==

print(num)

Output:

Skips only current iteration.



==============================
FILE: 2. class-seven.pdf
PAGE: 20
==============================

Combining Conditions & Loops
Real-world example:

values = [4, -2,

Vv values:
Vv <=

print(v)

Used frequently in:

e Data cleaning

e Feature filtering



==============================
FILE: 2. class-seven.pdf
PAGE: 21
==============================

Practical Example — Password Attempts

attempts =

while attempts <
password = input("Enter password: ")

if password == "admin123":
print("Access granted")
break

attempts +=

Demonstrates:

+ EEEIE) loop

e condition

ae break



==============================
FILE: 2. class-seven.pdf
PAGE: 22
==============================

Hands-On Exercise #1
Write a program to:

¢ Check if anumber is positive, negative, or zero



==============================
FILE: 2. class-seven.pdf
PAGE: 23
==============================

Hands-On Exercise #2
Write a loop that:

e Prints numbers from 1 to 20
¢ Skips multiples of 3
e Stops when number reaches 15



==============================
FILE: 2. class-seven.pdf
PAGE: 24
==============================

Common Beginner Errors

e Incorrect indentation

° Infinite loops
e Using wrong condition

+ Misplaced EREEM or

Debug by:

e Printing intermediate values
e Reading error messages carefully



==============================
FILE: 2. class-seven.pdf
PAGE: 25
==============================

Knowledge Check

1. Difference between and loop

2. What does do?

3. What happens if indentation is incorrect?

4. Write a condition to check if age is between 18 and 60

5. When would you use E@uysuited



==============================
FILE: 2. class-seven.pdf
PAGE: 26
==============================

Summary

¢ Control flow enables decision-making

om ifMelith control execution paths

° and enable repetition

° and refine loop behavior

¢ These concepts are foundational for ML programming



==============================
FILE: 2. class-seven.pdf
PAGE: 27
==============================

Next Class
Functions & Modular Programming

¢ Defining functions

e Parameters & return values
e Reusability

¢ Introduction to modules



==============================
FILE: 3. class-eight.pdf
PAGE: 1
==============================

Machine Learning
Python Programming Basics & Operators
Functions & Modular Programming

¢ Defining functions

e Parameters & return values
e Reusability

e Introduction to modules



==============================
FILE: 3. class-eight.pdf
PAGE: 2
==============================

Learning Objectives

By the end of this session students should be able to:

¢ Define and call Python functions

e Use positional, keyword, and default parameters

e Return single and multiple values; understand scope

e Write simple recursive functions with proper base cases

¢ Create, import, reload, and run modules (.py files) from the working folder



==============================
FILE: 3. class-eight.pdf
PAGE: 3
==============================

Why Functions & Modules?

e Functions encapsulate behavior: avoid repetition (DRY principle)
e Modules (.py files) package related code for reuse across projects

e Benefits: easier testing, clearer code, better maintainability



==============================
FILE: 3. class-eight.pdf
PAGE: 4
==============================

Parts of a Function

Part Purpose Example

Keyword to define a function def greet():
Name of function (snake_case) def calculate_total():

Inputs to the function (name, age)
docstring Description of what it does """Return greeting string."""

Send value back to caller return greeting




==============================
FILE: 3. class-eight.pdf
PAGE: 5
==============================

Function Basics — Syntax

function_name(param1, param2=default) :
"""OQptional docstring describing behavior.

won

result

Example:

greet(name) :

"""Return a greeting string."""
f"Hello, i"




==============================
FILE: 3. class-eight.pdf
PAGE: 6
==============================

Calling Functions — Simple Example

print(greet("Sita"))

Step-through:

e Python evaluates arguments, calls the function, executes body and returns result.



==============================
FILE: 3. class-eight.pdf
PAGE: 7
==============================

Parameters & Argument Types

e Positional arguments: order matters

e Keyword arguments: name=value (order independent)
¢ Default values: make arguments optional

e Variable-length: *args (positional), **kwargs (keyword)

Example:

power (3)
power(3, p




==============================
FILE: 3. class-eight.pdf
PAGE: 8
==============================

Returning Values

e Use return to send values back to the caller

e Multiple values returned as a tuple (can be unpacked)

Example:

min(values), max(values)

lo, hi = min_max([4, 1, 9])
print(lo, hi)




==============================
FILE: 3. class-eight.pdf
PAGE: 9
==============================

Variable Scope — Local vs Global

e Local: variables defined inside a function (not visible outside)
e Global: variables at module level (visible across the module)
e Prefer returning values instead of mutating globals

Example:

print(foo(), x)




==============================
FILE: 3. class-eight.pdf
PAGE: 10
==============================

Recursion — Concept & Example

e Recursion: a function calls itself to solve a smaller subproblem

e Always include a base case to stop recursion

Example (factorial):

n * factorial(n - 1)



==============================
FILE: 3. class-eight.pdf
PAGE: 11
==============================

Troubleshooting

Common Errors

Error

NameError: name

‘func’ is not defined

Cause

Function called before
definition

Solution

Define function before
using it

TypeError: func() missing 1 required
positional argument

Wrong number of
arguments

Check function signature

ModuleNotFoundError: No module named ‘xyz

Module file not found

Ensure .py file is in same
folder

RecursionError:
exceeded

maximum recursion depth

Missing/wrong base case

Add proper base case to
recursion

AttributeError:
‘func’

module has no attribute

Function not in module

Check module has the
function




==============================
FILE: 3. class-eight.pdf
PAGE: 12
==============================

Debugging Tips

print(

result = x * 2
print(

return result

import datautils
print(dir(datautils) )

print(

def (a, b):
return a + b

assert add(2, 3) == 5,
print( )




==============================
FILE: 3. class-eight.pdf
PAGE: 13
==============================

Modules — What & Why

e A module is a .py file containing functions, classes, and variables
¢ Import modules into other scripts or interactive sessions to reuse functionality
e Import styles:

°c import module — use module.name

° from module import name — use name directly



==============================
FILE: 3. class-eight.pdf
PAGE: 14
==============================

Create a Module File — Simple Approaches

A. Create using a text editor and save as mymodule.py in the project folder (recommended).
B. Programmatic write from Python (useful for quick examples):

open("mymodule.py", “w").write(code)

After saving the file in the same folder as your code, it can be imported as a module.



==============================
FILE: 3. class-eight.pdf
PAGE: 15
==============================

Example Module — datautils.py

(n):

return [i*i for i in range(1, n+1)]

(n):
if n <= 1:
return False
if n% 2 == @ andn != 2:
return False
i= 3
while i * i <= n:
if n% i == @:
return False
i+t=2
return True




==============================
FILE: 3. class-eight.pdf
PAGE: 16
==============================

(weight_kg, height_m):

bmi = weight_kg / (height_m ** 2)
if bmi < 18.5:
cat =
elif bmi < 25:
cat =
elif bmi < 30:
cat =
else:
cat =
return bmi, cat

, first_n_squares(5))
[n for n in range(1,21) if is_prime(n)])
, bmi_info(7®, 1.75))

,




==============================
FILE: 3. class-eight.pdf
PAGE: 17
==============================

Import and Use the Module

In another Python file or session within the same folder:

import datautils

print(datautils.first_n_squares(5) )
print([n for n in range(1,21) if datautils.is_prime(n)])
print(datautils.bmi_info(7@, 1.75))

Or import specific names:

from datautils import is_prime, bmi_info

print(is_prime(17) )
print(bmi_info(6@, 1.6))




==============================
FILE: 3. class-eight.pdf
PAGE: 18
==============================

Updating a Module During Development

e When you edit a module during the same interpreter/session, reload it:

importlib
importlib.reload(datautils)

e Alternatively, start a fresh interpreter/session to get a clean import.



==============================
FILE: 3. class-eight.pdf
PAGE: 19
==============================

Organizing Code — Simple Project Layout

project/
datautils.py

scripts/
run_demo. py
README. txt

Keep reusable modules at top-level of the project so other files can import them directly.



==============================
FILE: 3. class-eight.pdf
PAGE: 20
==============================

Best Practices & Common Pitfalls

¢ Keep functions focused and small

Add docstrings and short comments for public functions
e Avoid heavy computation at import time (use demo/test blocks)

Rm ASR e import module) Ream from module import name

For mutable default arguments use None and initialize inside the function

Example (safe default pattern):

(

1st None:

Ist = []
lst .append(x)
1st




==============================
FILE: 3. class-eight.pdf
PAGE: 21
==============================

Hands-on Exercises (in-class / lab)

1. Create with the example functions. Run the file's quick checks to verify
outputs.

2. In a separate file, import and call each function for sample inputs.
3. Improve for small optimizations (handle even numbers, skip even divisors).
4. Add simple assertion tests in a small test file to validate behavior.



==============================
FILE: 3. class-eight.pdf
PAGE: 22
==============================

Assessment / Homework

¢ Deliverables:
o CEyetemcmea With required functions and docstrings

© ashort script showing how you import and use the module (e.g., Eelgieresyag mete ="q )
° listing exact steps you used to run the demo in your environment

Evaluation will check correctness, code clarity, and presence of basic tests.



==============================
FILE: 1. class-nine.pdf
PAGE: 1
==============================

Machine Learning

Week 3 - Class 1

Introduction to Linear Regression



==============================
FILE: 1. class-nine.pdf
PAGE: 2
==============================

Class Objective
By the end of this class, students will be able to:

e¢ Understand what regression is

e Explain the idea of linear regression

e Identify dependent and independent variables

¢ Derive the linear regression equation

¢ Perform manual calculations for simple linear regression

e Interpret slope and intercept meaningfully



==============================
FILE: 1. class-nine.pdf
PAGE: 3
==============================

What Is Regression?
Regression is a supervised learning technique where:

e¢ Output variable is continuous

e The goal is to predict a numerical value
Examples:

e Predict house price
e Predict salary from experience

e Predict fuel consumption from engine size



==============================
FILE: 1. class-nine.pdf
PAGE: 4
==============================

Why Linear Regression?
Linear regression is used when:

e Relationship between variables is approximately linear
e We want a simple, interpretable model
e¢ Data follows a straight-line trend

It is:

e Easy to understand
e Easy to compute
e Widely used as a baseline model



==============================
FILE: 1. class-nine.pdf
PAGE: 5
==============================

Real-World Uses of Linear Regression

e Market research & customer surveys

e Automobile engine performance analysis
e Pricing of goods and services

e Astronomy and scientific measurements

¢ Economics and finance forecasting



==============================
FILE: 1. class-nine.pdf
PAGE: 6
==============================

Simple Linear Regression
Simple Linear Regression has:

¢ One independent variable (X)
¢ One dependent variable (Y)

General form:

Where:

¢ m= slope

e c= intercept

Y=mX+c



==============================
FILE: 1. class-nine.pdf
PAGE: 7
==============================

Understanding the Variables

¢ Independent variable (X)
— input, predictor, feature

e¢ Dependent variable (Y)
— output, response, target

Example:

e X = hours studied
e Y =exam score



==============================
FILE: 1. class-nine.pdf
PAGE: 8
==============================

Graphical Interpretation

¢ X axis + Independent variable
e Y axis ~ Dependent variable
¢ Best-fit straight line represents the model

e¢ Objective: minimize error between actual and predicted values



==============================
FILE: 1. class-nine.pdf
PAGE: 9
==============================

Problem Statement (Example)

Consider the following data:

X (Hours Studied) Y (Marks)

al 2

4
3)
4
6

wm} BR] @) Nd

We want to find the best fit line.



==============================
FILE: 1. class-nine.pdf
PAGE: 10
==============================

Step 1: Formula for Slope (m)
my XY —(X)OUY)

ny X?- (SX)?
Where:

¢ n= number of observations



==============================
FILE: 1. class-nine.pdf
PAGE: 11
==============================

Step 2: Compute Required Values

x

XY

x<
<
i)

1 2 i
2 4

3 5 9 15
4 4 16 16
5 6 25 30




==============================
FILE: 1. class-nine.pdf
PAGE: 12
==============================

Summations

SOX =15

«> ¥ =21

sy) X55
e SAY =7TI
en=5d



==============================
FILE: 1. class-nine.pdf
PAGE: 13
==============================

Step 3: Calculate Slope (m)
5(71) — (15) (21)
m=
5(55) — (15)2
mess lb
1) O75 — 225
=5 =

m 0.8



==============================
FILE: 1. class-nine.pdf
PAGE: 14
==============================

Step 4: Formula for Intercept (c)

c= SY-myx

n

Substitute values:

21 — (0.8 x 15)
—
5

Zl = I 9)
S=j= 18
5) 5




==============================
FILE: 1. class-nine.pdf
PAGE: 15
==============================

Final Regression Equation
Y =0.8X + 1.8
This is the best fit line for the given data.



==============================
FILE: 1. class-nine.pdf
PAGE: 16
==============================

Interpretation of the Model

¢ Slope (0.8)

— For every 1 unit increase in X, Y increases by 0.8 units
¢ Intercept (1.8)

— Expected value of Y when X = 0



==============================
FILE: 1. class-nine.pdf
PAGE: 17
==============================

Making Predictions

lf a student studies for 6 hours:
Y = 0.8(6) + 1.8 = 6.6

Predicted marks = 6.6



==============================
FILE: 1. class-nine.pdf
PAGE: 18
==============================

Error in Linear Regression
Error (Residual):

Error — Yactnal — Ypredicted
Goal of linear regression:

e Minimize total error

e Best fit line minimizes sum of squared errors



==============================
FILE: 1. class-nine.pdf
PAGE: 19
==============================

Limitations of Linear Regression

e Assumes linear relationship
e Sensitive to outliers
¢ Cannot model complex curves

e Poor performance if data is non-linear



==============================
FILE: 1. class-nine.pdf
PAGE: 20
==============================

What Comes Next?

Next classes will cover:

e¢ Multiple Linear Regression
e¢ Polynomial Regression

e¢ Matrix methods

e Error metrics (MSE, RMSE)



==============================
FILE: 1. class-nine.pdf
PAGE: 21
==============================

In-Class Exercise

1. Use the given formula to compute regression for a new dataset
2. Plot the data points and regression line
3. Predict output for unseen values



==============================
FILE: 1. class-nine.pdf
PAGE: 22
==============================

Practice Problem

A company wants to predict Sales (Y) based on Advertising Budget (X) in thousands of dollars.

X (Budget) Y (Sales)

2 g]

83 5)

4 6

5) 8

6 9
Tasks:

1. Calculate the regression equation
2. Predict sales when budget = 7



==============================
FILE: 2. class-ten.pdf
PAGE: 1
==============================

Machine Learning

Week 3 - Class 2

Simple Linear Regression Using Python



==============================
FILE: 2. class-ten.pdf
PAGE: 2
==============================

Class Objective
By the end of this class, students will be able to:

¢ Load CSV data using NumPy and Pandas

¢ Inspect and understand tabular datasets
e Plot data points using Matplotlib

e Apply Simple Linear Regression in Python
e Visualize the regression line

e Interpret model output (slope & intercept)



==============================
FILE: 2. class-ten.pdf
PAGE: 3
==============================

Recap: What We Did Last Class

Previously, we:

e Defined linear regression

¢ Derived the regression equation manually
e Calculated slope and intercept

e Predicted values mathematically

Today: We let Python do the same work programmatically.



==============================
FILE: 2. class-ten.pdf
PAGE: 4
==============================

Dataset Used

We will use a CSV file containing:

Budget (X) Sales (Y)

2 3
3 5
4 6
5 8
6 9

e X — Advertising Budget (in thousands)
e Y — Sales (in thousands)



==============================
FILE: 2. class-ten.pdf
PAGE: 5
==============================

Step 1: Required Libraries
We will use the following Python libraries:

¢ NumPy ~> numerical computation

¢ Pandas — data loading & manipulation
e Matplotlib — visualization

¢ Scikit-learn — regression model



==============================
FILE: 2. class-ten.pdf
PAGE: 6
==============================

Importing Libraries

numpy np
pandas pd

matplotlib.pyplot plt
sklearn.linear_model LinearRegression




==============================
FILE: 2. class-ten.pdf
PAGE: 7
==============================

Step 2: Loading CSV Data Using Pandas
Assume the file is named EEICRIRERe eng.

data = pd.read_csv("s

print(data)




==============================
FILE: 2. class-ten.pdf
PAGE: 8
==============================

Inspecting the Dataset

data.head()

data.info()

data.describe()

Purpose:

e Understand structure
e Check data types

e Look for missing values



==============================
FILE: 2. class-ten.pdf
PAGE: 9
==============================

Separating Independent and Dependent Variables




==============================
FILE: 2. class-ten.pdf
PAGE: 10
==============================

Step 3: Visualizing the Data (Scatter Plot)

.scatter(X, Y)
.xlabel(

.ylabel(
.title(
. show()

This plot shows the linear relationship between X and Y.



==============================
FILE: 2. class-ten.pdf
PAGE: 11
==============================

Step 4: Creating the Linear Regression Model

model = LinearRegression()
model.fit(X, Y)

Train (fit) the model on the data.



==============================
FILE: 2. class-ten.pdf
PAGE: 12
==============================

Extracting Model Parameters

model .coef_[@]
model .intercept_

m
c

print( , m)
print( , Cc)

These values correspond to the formula:

Y=mX+c



==============================
FILE: 2. class-ten.pdf
PAGE: 13
==============================

Comparing With Manual Calculation
From previous class:

e¢ Manual slope = 1.5
e Manual intercept ~ 0.2

Python output should closely match these values.



==============================
FILE: 2. class-ten.pdf
PAGE: 14
==============================

Step 5: Plotting Regression Line

Y_pred = model.predict(X)

plt.scatter(X, Y, label="Actual Data")
plt.plot(X, Y_pred, color="red", label="Regression Line")

plt.xlabel("Advertising Budget")
plt.ylabel("“Sales")

plt.legend()

plt.show()




==============================
FILE: 2. class-ten.pdf
PAGE: 15
==============================

Making Predictions Using the Model
Predict sales when budget = 7:

new_budget = np.array([[7]])
prediction = model.predict (new_budget )

print("Pred 3:", prediction[@])




==============================
FILE: 2. class-ten.pdf
PAGE: 16
==============================

Interpretation of the Output

¢ Slope (m)

— Increase in sales for every unit increase in budget
e Intercept (c)

— Expected sales when budget = 0

This matches our theoretical understanding from Class 1.



==============================
FILE: 2. class-ten.pdf
PAGE: 17
==============================

Error Concept (Brief)

errors = Y - Y_pred

Later we will study:

e Mean Squared Error (MSE)
e Root Mean Squared Error (RMSE)



==============================
FILE: 2. class-ten.pdf
PAGE: 18
==============================

Why Use Libraries Instead of Manual Calculation?

e Handles large datasets
e Less error-prone
e Faster computation

e Industry standard practice

Manual calculation is important for understanding, not for production.



==============================
FILE: 2. class-ten.pdf
PAGE: 19
==============================

In-Class Exercise

1. Load the CSV file provided

2. Plot the dataset

3. Train a linear regression model
4. Print slope and intercept

5. Predict output for a new input



==============================
FILE: 2. class-ten.pdf
PAGE: 20
==============================

Homework

¢ Complete the in-class exercise

¢ Write a full Python script from data loading to prediction
e Test with the practice dataset from Class 1

e Prepare for Multiple Linear Regression



==============================
FILE: 3. class-eleven.pdf
PAGE: 1
==============================

Decision Tree
Class 11-ID3 Algorithm



==============================
FILE: 3. class-eleven.pdf
PAGE: 2
==============================

Class Objective
By the end of this class, students will be able to:

Understand what a Decision Tree is

Explain the ID3 algorithm

Calculate Entropy
Calculate Information Gain

Select the root node manually



==============================
FILE: 3. class-eleven.pdf
PAGE: 3
==============================

What Is a Decision Tree?
A Decision Tree is a supervised learning model that:

e Predicts output using decision rules
e Splits data based on attribute values
e Has:

© Decision nodes — attributes

o Leaf nodes — class labels

Used mainly for classification.



==============================
FILE: 3. class-eleven.pdf
PAGE: 4
==============================

Example

Is the person fit?




==============================
FILE: 3. class-eleven.pdf
PAGE: 5
==============================

ID3 Algorithm
ID3 stands for Iterative Dichotomiser 3
Key points:

e Top-down approach

e Greedy algorithm

e Selects attribute with maximum Information Gain
e¢ Works best with categorical data



==============================
FILE: 3. class-eleven.pdf
PAGE: 6
==============================

How Does ID3 Choose a Split?
ID3 uses:
Information Gain
The attribute that reduces uncertainty the most is chosen as the node.

To calculate Information Gain, we first need Entropy.



==============================
FILE: 3. class-eleven.pdf
PAGE: 7
==============================

●
All students pass → No confusion → Entropy = 
0
●
Half pass, half fail → Maximum confusion


==============================
FILE: 3. class-eleven.pdf
PAGE: 8
==============================

Information Gain (Theory)
Information Gain measures:
Reduction in entropy after splitting on an attribute

Formula:
Gain(S, A) = Entropy(S) — SS os Entropy(S,)

Where:

e S = Original dataset
e A= Attribute to split on
e §_v = Subset v after splitting by attribute A

In short:

1. Find entropy before split

e |S_v| = Number of records in subset v 2. Find entropy after split

e |S] = Total number of records in S 2 BESTE

e = Sum over all subsets created by splitting on A



==============================
FILE: 3. class-eleven.pdf
PAGE: 9
==============================

Numerical Example Dataset
Target: Play Tennis (Yes / No)

S. No. Outlook Temperature Humidity

1
2
3
4
5
6
7
8
9
10
11
12
13
14




==============================
FILE: 3. class-eleven.pdf
PAGE: 10
==============================

Step 1: Entropy of Dataset
Total records = 14

e Yes=9
e No=5

5

$) ) 5
Entropy(S) = — 7g l08e re 74 W082 Ai

Entropy(S) = 0.94



==============================
FILE: 3. class-eleven.pdf
PAGE: 11
==============================

Step 2: Split on Attribute - Outlook

Outlook Yes No Total
Sunny
Overcast | 4 jo |4 |
Ran [3 |2 [5 |




==============================
FILE: 3. class-eleven.pdf
PAGE: 12
==============================

Entropy for Each Split

Sunny:
2 ?} 3) 3
Entropy = 5 082 5 5 082 == 0.97
Overcast:
Entropy =0 (pure)
Rain:

Entropy = 0.97



==============================
FILE: 3. class-eleven.pdf
PAGE: 13
==============================

Step 3: Information Gain (Outlook)

: 5 4 5
Gain(S, Outlook) = 0.94 — € x 0.97 + Satis O+ 1a 097)

Gain(S, Outlook) = 0.246



==============================
FILE: 3. class-eleven.pdf
PAGE: 14
==============================

Root Node Selection

e Calculate Information Gain for all attributes
e Attribute with highest gain becomes:

Root Node
»> Outlook



==============================
FILE: 3. class-eleven.pdf
PAGE: 15
==============================

Building the Decision Tree

Outlook (Root)

/ | \
/ | N
Sunny Overcast Rain
/ | i
i} | / \
? YES ? ?
if \
/ \
NO YES
After splitting on Outlook:

e Overcast — Pure (all YES) V
e Sunny & Rain — Need further splits



==============================
FILE: 3. class-eleven.pdf
PAGE: 16
==============================

Next Step: Analyze Sunny Node

Sunny subset: 5 records (2 Yes, 3 No)
Entropy(Sunny) = 0.97

Need to check other attributes:
e Temperature, Humidity, Wind, etc.

Calculate Information Gain for each attribute within Sunny subset.



==============================
FILE: 3. class-eleven.pdf
PAGE: 17
==============================

Next Step: Analyze Rain Node

Rain subset: 5 records (3 Yes, 2 No)
Entropy(Rain) = 0.97

Need to check other attributes to further split.

Example: Wind might be the best split here



==============================
FILE: 3. class-eleven.pdf
PAGE: 18
==============================

ID3 Algorithm Steps (Exam Answer)

1. Calculate entropy of dataset
2. Calculate information gain for each attribute
3. Select attribute with highest gain
4. Split dataset
5. Repeat until:
o All nodes are pure OR
© No attributes left



==============================
FILE: 3. class-eleven.pdf
PAGE: 19
==============================

Partial Decision Tree Summary

Outlook [Gain = 0.246]

/ | \
M | \\
Sunny Overcast Rain
(2YemeoN)) (4Y, ON) (BYan2N)
| | |
? YES ?

(Need split)

e Overcast is pure (leaf node)
e Sunny and Rain need further analysis



==============================
FILE: 3. class-eleven.pdf
PAGE: 21
==============================

Key Takeaways

Y ID3 builds trees top-down using greedy approach

¥ Information Gain determines which attribute to split on
Y Process repeats recursively on subsets

¥ Stops when all nodes are pure or no attributes remain

Next class: Complete decision tree + Handling continuous attributes



==============================
FILE: 4. class-thirteen.pdf
PAGE: 1
==============================

Machine Learning

Naive Bayes Classifier

Mathematical Classification



==============================
FILE: 4. class-thirteen.pdf
PAGE: 2
==============================

Class Objective
By the end of this class, students will be able to:

¢ Apply Bayes Theorem for classification

e Use the Naive (conditional independence) assumption
¢ Compute posteriors step-by-step for a new instance
e Classify an unseen data point mathematically



==============================
FILE: 4. class-thirteen.pdf
PAGE: 3
==============================

Key terms

e Prior: a starting guess for how likely each class is, before seeing the new example.

e Likelihood: how likely the observed features are under a given class (from the training
data).

¢ Posterior: the updated probability of a class after seeing the example (what we want).
e Normalize: scaling scores so they add to 1 and become proper probabilities.



==============================
FILE: 4. class-thirteen.pdf
PAGE: 4
==============================

Bayes Theorem (Core Idea)

For class v and evidence 2:

v) = prior probability of class
x | v) = likelihood
v | x) = posterior probability

x) = evidence (normalization)



==============================
FILE: 4. class-thirteen.pdf
PAGE: 5
==============================

Naive Bayes Assumption

If c = (a1, 42,...,@,), then:

n
Ge | O) = [[ PCa |»)
i=1

So the decision rule becomes:

UNB = arg ae P(v) [] PC: i)

Naive Bayes assumes each feature (like Outlook, Temperature) gives independent evidence
about the class. We compute a score for each class by multiplying the prior and the feature
probabilities, then pick the class with the higher score.



==============================
FILE: 4. class-thirteen.pdf
PAGE: 6
==============================

Dataset

Outlook Temperature Humidity Windy PlayTennis

Sunny Hot High | Weak Ne)
Sunny Hot High | Strong No
Overcast Hot High | Weak Yes
Rainy Mild High | Weak Yes
Rainy Cool Normal | Weak Yes
Rainy Cool Normal | Strong Ne)
Overcast Cool Normal | Strong Yes
Sunny Mild High | Weak No
Sunny Cool Normal | Weak Yes
Rainy Mild Normal | Weak Yes




==============================
FILE: 4. class-thirteen.pdf
PAGE: 7
==============================

Outlook Temperature Humidity Windy PlayTennis

Sunny Mild Normal | Strong Yes
Overcast Mild High | Strong Yes
Overcast Hot Normal | Weak Yes
Rainy Mild High | Strong Ne)




==============================
FILE: 4. class-thirteen.pdf
PAGE: 8
==============================

Dataset: PlayTennis (Summary)
Class counts (14 total):

e PlayTennis = Yes: 9
e PlayTennis = No:5

So:



==============================
FILE: 4. class-thirteen.pdf
PAGE: 9
==============================

Conditional Probabilities (From Training Data)

Outlook

Temperature

Outlook P(-| Yes) P(-| No)

sunny 2/9 3/5
overcast 4/9 0/5
rain 3/9 2/5

Temp P(-| Yes) P(-| No)

hot 2/9 2/5
mild 4/9 2/5
cool 3/9 1/5




==============================
FILE: 4. class-thirteen.pdf
PAGE: 10
==============================

Conditional Probabilities (Continued)

Humidity

Humidity P(-| Yes) P(-| No)
high 3/9 4/5

normal 6/9 1s

Wind

Wind P(-| Yes) P(- | No)
strong 3/9 3/5
weak 6/9 2/5




==============================
FILE: 4. class-thirteen.pdf
PAGE: 11
==============================

Problem Instance
Classify:
xz = (Outlook = sunny, Temperature = cool, Humidity = high, Wind = strong)

We compute unnormalized scores:

Score(v) = P(v) [[P@ i)

These are not yet probabilities — they are proportional to the posterior. We divide by the sum
of scores to get real probabilities that sum to 1.



==============================
FILE: 4. class-thirteen.pdf
PAGE: 12
==============================

Score for Yes (Step-by-step)
Score(Yes) = P(Yes) - P(sunny | Yes) - P(cool | Yes) - P(high | Yes) - P(strong | Yes)
Substitute values:

Dal, See nae



==============================
FILE: 4. class-thirteen.pdf
PAGE: 13
==============================

Score for No (Step-by-step)
Score(No) = P(No) - P(sunny | No) - P(cool | No) - P(high | No) - P(strong | No)

Substitute values:



==============================
FILE: 4. class-thirteen.pdf
PAGE: 14
==============================

Normalize to Get Posterior Probabilities

Score(Yes)
P(Y =
(Yes| 2) Score(Yes) + Score(No)
N
P(No|«) = Score(No)

Score(Yes) + Score(No)
Using computed scores:

+ P(Yes | x) = 42 ~ 0.2046
* P(No| a) = 8% ~ 0.7954

Y Prediction: PlayTennis = No



==============================
FILE: 4. class-thirteen.pdf
PAGE: 15
==============================

New Unseen Instance
Now classify a new data point:
Lnew = (Outlook = rain, Temperature = mild, Humidity = high, Wind = weak)

Compute scores again.



==============================
FILE: 4. class-thirteen.pdf
PAGE: 16
==============================

New Instance: Score(Yes)

Score(Yes) = a



==============================
FILE: 4. class-thirteen.pdf
PAGE: 17
==============================

New Instance: Score(No)



==============================
FILE: 4. class-thirteen.pdf
PAGE: 18
==============================

New Instance: Normalize + Decision

0.02116
LAG = = 0.5365
(Yes | tnew) = 995776 + 0.01829

P(No | tnew) © 0.4635

Prediction for unseen data: PlayTennis = Yes



==============================
FILE: 4. class-thirteen.pdf
PAGE: 19
==============================

Summary

e Naive Bayes uses:

° Priors P(v)

© Likelihoods P(a; | v)

© Product rule (naive independence)
¢ For PPT instance (sunny, cool, high, strong): No
¢ For unseen instance (rain, mild, high, weak): Yes



==============================
FILE: class-fifteen.pdf
PAGE: 1
==============================

Machine Learning
K-Nearest Neighbors (KNN)

Supervised Learning - Classification



==============================
FILE: class-fifteen.pdf
PAGE: 2
==============================

Class Objectives
By the end of this class, students will be able to:

e Understand what K-Nearest Neighbors (KNN) is

e Explain why KNN is a lazy learning algorithm

e Select an appropriate value of K

¢ Perform KNN classification numerically step-by-step
e Identify advantages and limitations of KNN



==============================
FILE: class-fifteen.pdf
PAGE: 3
==============================

What Is K-Nearest Neighbors (KNN)?

K-Nearest Neighbors (KNN) is a supervised learning algorithm that:

e Stores all training data
e Classifies a new data point based on similarity
e Uses distance measures (most commonly Euclidean distance)

No explicit training phase — prediction happens at runtime.



==============================
FILE: class-fifteen.pdf
PAGE: 4
==============================

Why Is KNN Called a Lazy Algorithm?

KNN is a lazy learner because:

e It does not build a model
e It simply memorizes training data
e¢ Computation is deferred until prediction time

Where as, Eager learners like Decision Trees, build a model during training.



==============================
FILE: class-fifteen.pdf
PAGE: 5
==============================

Distance Measure Used in KNN
Most commonly used distance: Euclidean Distance
For two points x = (x1, 22) and a = (aj, a2):
d(x, a) = 4/ (x1 — a1)? + (a2 — a2)?
Squared Euclidean Distance (often used to avoid square roots):

d?(x, a) = (x — a,)° + (a2 —- ap)*



==============================
FILE: class-fifteen.pdf
PAGE: 6
==============================

How to Choose the Value of K?
Rule of Thumb

e Start withk = 1
e Increase k gradually

¢ Choose k with minimum error on a validation set
Practical Guidelines

e Use odd values of k (to avoid ties)
e¢ Small k > Overfitting
e Large k > Underfitting



==============================
FILE: class-fifteen.pdf
PAGE: 7
==============================

Overfitting v/s Underfitting

Aspect

Overfitting

Model learns training data too well,

Underfitting

Model is too simple to capture

Silat including noise patterns
Training Error | Very low High
Test Error High High
Bias Low High
Variance High Low
Generalization | Poor Poor

Simple
Example

Memorizing exam questions instead of
understanding concepts

Using only addition to solve all
math problems




==============================
FILE: class-fifteen.pdf
PAGE: 8
==============================

Bias and Variance
Bias
The error that occurs when a model is too simple to capture the true patterns in the data.

e High bias: The model oversimplifies, misses patterns and underfits the data.

e Low bias: The model captures patterns well and is closer to the true values.
Variance

Variance arises when a model becomes too sensitive to training data and it captures noises in
data too. It fails to give prediction on unseen new data.

e High variance: The model is too sensitive to small changes and may overfit.
e Low variance: The model is more stable but might miss some patterns.

Goal: Find the right balance between bias and variance for optimal generalization.



==============================
FILE: class-fifteen.pdf
PAGE: 9
==============================

Effect of K on Model Behavior
Small k

e Low bias

e High variance

e Sensitive to noise
e Risk of overfitting

Large k

e High bias

e Low variance

e¢ Smoother decision boundary
e Risk of underfitting



==============================
FILE: class-fifteen.pdf
PAGE: 10
==============================

KNN Algorithm - Step by Step

1. Choose the value of k

2. Compute distance between query point and all training points
3. Sort distances in ascending order

4. Select k nearest neighbors

5. Take majority vote (classification)

6. Assign the predicted class



==============================
FILE: class-fifteen.pdf
PAGE: 11
==============================

Numerical Example - Problem Statement

Attributes:

e X, = Acid Durability
e X» = Strength

Target variable:
e Good / Bad

Query (new sample):
AG =O, Xo =

Goal: Predict whether the tissue is Good or Bad



==============================
FILE: class-fifteen.pdf
PAGE: 12
==============================

Step 1: Choose k
let k= 3:

Note: Odd value avoids class ties.



==============================
FILE: class-fifteen.pdf
PAGE: 13
==============================

Training Data (Labeled Samples)

Sample X, (Acid) X»> (Strength) Class

A 2 7 Good
B 8) Good
Cc 4 8 Bad
D 5 bs) Bad
E a 4 Good
F 7 i Bad
G 3 8 Bad




==============================
FILE: class-fifteen.pdf
PAGE: 14
==============================

Step 2: Compute Distances
Use Squared Euclidean Distance to avoid square roots:

d? (3,7), (a1, a2)) = (3 — a1)? + (7 — a)?

Sample Point (a, a2)

iN (27) (3 — 2)? + (7-7)? =1 | Good
B (3, 6) (3 — 3)? + (7-6)? =1 | Good
fe (3, 8) (3 —3)?+(7-—8)?=1 | Bad
Cc (4, 8) (3 —4)?+ (7-8)? =2 | Bad
D (5, 5) (3 —5)?+ (7-5)? =8 | Bad
E (1, 4) (3 — 1)? + (7-4)? = 13) Good
F (7, 7) (3 — 7)? + (7-7)? =16| Bad




==============================
FILE: class-fifteen.pdf
PAGE: 15
==============================

Step 3: Sort Distances (Ascending)
Order of samples by increasing d?:

1. A (1, Good), B (1, Good), G (1, Bad)
2. C (2, Bad)

3. D (8, Bad)

4. E (13, Good)

5. F (16, Bad)



==============================
FILE: class-fifteen.pdf
PAGE: 16
==============================

Step 4: Select k Nearest Neighbors (k = 3)
Nearest 3 neighbors: A, B, G

¢ Neighbor 1 > A (Good)
e Neighbor 2 — B (Good)
e Neighbor 3 — G (Bad)



==============================
FILE: class-fifteen.pdf
PAGE: 17
==============================

Step 5: Majority Voting (k = 3)
Count class labels among A, B, G:

¢ Good = 2
e Bad=1

Majority class = Good



==============================
FILE: class-fifteen.pdf
PAGE: 18
==============================

Final Prediction (for k = 3)
The new paper tissue with X; = 3, X2 = 7is classified as:

fefete})



==============================
FILE: class-fifteen.pdf
PAGE: 19
==============================

Comparison with k = 5
Select nearest 5 neighbors: A, B, G, C, D

e Good: A, B > 2
e Bad: G,C,D > 3

Result for k = 5: BAD

This shows how increasing k can change the prediction by smoothing decision boundaries and
reducing sensitivity to very-close points.



==============================
FILE: class-fifteen.pdf
PAGE: 20
==============================

Applications of KNN

e Classification problems

e Missing value estimation

e Pattern recognition

¢ Document similarity

e Gene expression analysis

e Image and handwriting recognition



==============================
FILE: class-fifteen.pdf
PAGE: 21
==============================

Practice

e Recompute predictions for a different query, e.g., (X1, X2) = (4,7), fork = 3andk=5.



==============================
FILE: class-sixteen.pdf
PAGE: 1
==============================

Machine Learning
Model Evaluation Metrics

Understanding Model Performance



==============================
FILE: class-sixteen.pdf
PAGE: 2
==============================

Class Objectives

By the end of this class, students will be able to:

Interpret a Confusion Matrix

Calculate and distinguish between Accuracy and Error Rate
Understand Precision, Recall, and their trade-offs
Compute and interpret the F1-Score

Evaluate regression models using MAE, MSE, RMSE
Understand and calculate R? (Coefficient of Determination)
Choose the appropriate metric for different scenarios



==============================
FILE: class-sixteen.pdf
PAGE: 3
==============================

Why Model Evaluation Metrics?

Model evaluation metrics help us answer:

¢ How well is my model performing?

e Is my model making the right predictions?
¢ Which model should I choose?

¢ What type of errors is my model making?

Different metrics serve different purposes — choosing the right one depends on your problem
domain and business requirements.



==============================
FILE: class-sixteen.pdf
PAGE: 4
==============================

1. Confusion Matrix
Definition

A confusion matrix is a table that visualizes the performance of a classification model by
comparing actual vs. predicted values.

For binary classification:

Predicted Positive Predicted Negative

Actual Positive | TP (True Positive) | FN (False Negative)
Actual Negative | FP (False Positive) | TN (True Negative)




==============================
FILE: class-sixteen.pdf
PAGE: 5
==============================

Confusion Matrix Terms

e True Positive (TP): Correctly predicted positive class

e True Negative (TN): Correctly predicted negative class

¢ False Positive (FP): Incorrectly predicted positive (Type | Error)

e False Negative (FN): Incorrectly predicted negative (Type II Error)

Example: Email spam detection

e TP: Spam correctly classified as spam

e TN: Not spam correctly classified as not spam
e FP: Not spam incorrectly classified as spam

e FN: Spam incorrectly classified as not spam



==============================
FILE: class-sixteen.pdf
PAGE: 6
==============================

Confusion Matrix Example

Given predictions for 100 emails:

Predicted Spam Predicted Not Spam

Actual Spam 40 (TP) 10 (FN)
Actual Not Spam 5 (FP) 45 (TN)

e¢ Total Spam emails: 40 + 10 = 50
e Total Not Spam emails: 5 + 45 = 50
e Total predictions: 100



==============================
FILE: class-sixteen.pdf
PAGE: 7
==============================

2. Accuracy
Definition

Accuracy measures the proportion of correct predictions out of total predictions.

Formula
A TP+TN
Tr —
COUracy ~ TP4+TN +FP+EN
Or simply:
Number of Correct Predictions
Accuracy =

Total Number of Predictions



==============================
FILE: class-sixteen.pdf
PAGE: 8
==============================

Accuracy Calculation Example

Using our spam detection confusion matrix:

TP+TN 40 + 45 85

TP+TNLFPLFN 4044545410 107 °°

Accuracy =

Accuracy = 85%

The model correctly classified 85 out of 100 emails.



==============================
FILE: class-sixteen.pdf
PAGE: 9
==============================

When to Use Accuracy?
Use when:

¢ Classes are balanced (roughly equal distribution)

e All types of errors are equally important
Do NOT use when:

e Classes are imbalanced (e.g., 95% negative, 5% positive)
¢ Different types of errors have different costs

Example of misleading accuracy:
lf 95% of emails are not spam, a model that always predicts "not spam" achieves 95% accuracy
but is useless!



==============================
FILE: class-sixteen.pdf
PAGE: 10
==============================

Error Rate

Definition

Error Rate (also called Misclassification Rate) is the proportion of incorrect predictions.
Formula

FP+FN

E S c°o__-.5C_ $C. "\_ CO“
rror Rate = 7p TN 1 FP LEN

Error Rate = 1 — Accuracy



==============================
FILE: class-sixteen.pdf
PAGE: 11
==============================

Error Rate Calculation Example

Using our spam detection confusion matrix:

FP+FN _ 5 +10

E te = 2 _——_ =
rror Rate {ADI IMNpae (Doosan 100

Error Rate = 1 — 0.85 = 0.15
Error Rate = 15%

The model incorrectly classified 15 out of 100 emails.

15
= —— = (0.1
100 oe



==============================
FILE: class-sixteen.pdf
PAGE: 12
==============================

3. Precision

Definition

Precision measures the proportion of positive predictions that were actually correct. It
answers: "Of all the instances we predicted as positive, how many were truly positive?"

Formula

TP

Precision = TP+FP

Precision focuses on minimizing False Positives.



==============================
FILE: class-sixteen.pdf
PAGE: 13
==============================

Precision Calculation Example

Using our spam detection confusion matrix:

TP 40 ~~ 40
TP+FP 40+5 45

Precision = = 0.889

Precision = 88.9%

Of all emails predicted as spam, 88.9% were actually spam.
11.1% were false alarms (legitimate emails marked as spam).



==============================
FILE: class-sixteen.pdf
PAGE: 14
==============================

When to Use Precision?
Use when:

e False Positives are costly
e You want to be confident when you predict positive

Examples:

e¢ Email spam filter: False Positive means important email goes to spam
e Medical diagnosis: False Positive means healthy person gets treatment
e Criminal conviction: False Positive means innocent person convicted

In these cases, we prefer to miss some positive cases rather than incorrectly flag negatives as
positive.



==============================
FILE: class-sixteen.pdf
PAGE: 15
==============================

4. Recall (Sensitivity/True Positive Rate)

Definition

Recall measures the proportion of actual positives that were correctly identified. It answers:
“Of all the actual positive instances, how many did we correctly identify?"

Formula

TP

i = ———__.
Reca TP + FN

Recall focuses on minimizing False Negatives.



==============================
FILE: class-sixteen.pdf
PAGE: 16
==============================

Recall Calculation Example

Using our spam detection confusion matrix:

TP 40 40

TP+ FN 4+10 7 507 °°

Recall =

Recall = 80%

Of all actual spam emails, we correctly identified 80%.
We missed 20% of spam emails (they went to inbox).



==============================
FILE: class-sixteen.pdf
PAGE: 17
==============================

When to Use Recall?
Use when:

e False Negatives are costly
e You want to catch as many positive cases as possible

Examples:

e Disease detection: False Negative means sick person not diagnosed
e Fraud detection: False Negative means fraud goes undetected
e Airport security: False Negative means threat not detected

In these cases, we prefer some false alarms over missing actual positive cases.



==============================
FILE: class-sixteen.pdf
PAGE: 18
==============================

Precision vs. Recall Trade-off
There is often a trade-off between Precision and Recall:

¢ High Precision, Low Recall: Model is conservative, only predicts positive when very
confident (few false alarms, but misses many positives)

¢ Low Precision, High Recall: Model is aggressive, predicts positive liberally (catches most
positives, but many false alarms)

You cannot maximize both simultaneously — you must choose based on your problem
requirements.



==============================
FILE: class-sixteen.pdf
PAGE: 19
==============================

Precision-Recall Example Comparison
Scenario 1: Strict Spam Filter (High Precision)

¢ TP=30, FP=2, FN=20, TN=48

e Precision = 30/(30+2) = 93.75%

* Recall = 30/(30+20) = 60%

e Few false alarms, but misses many spam emails
Scenario 2: Aggressive Spam Filter (High Recall)

e TP=45, FP=15, FN=5, TN=35
e Precision = 45/(45+15) = 75%
e Recall = 45/(45+5) = 90%

e Catches most spam, but more false alarms



==============================
FILE: class-sixteen.pdf
PAGE: 20
==============================

5.F1-Score
Definition

The F1-Score is the harmonic mean of Precision and Recall. It provides a single metric that
balances both concerns.

Formula
Precision x Recall
F1-S =2x ———___——_
core “ Precision + Recall
Or equivalently:
2xTP
F1-Score =

2xTP+FP+ FN



==============================
FILE: class-sixteen.pdf
PAGE: 21
==============================

F1-Score Calculation Example
Using our spam detection confusion matrix:

e Precision = 0.889
e Recall = 0.80

0.889 x 0.80 _ . 0.7112
0.889 +0.80 1.689

F1-Score = 2 x = 2 x 0.421 = 0.842

F1-Score = 84.2%



==============================
FILE: class-sixteen.pdf
PAGE: 22
==============================

Why Harmonic Mean?
The harmonic mean is used instead of arithmetic mean because:

e It penalizes extreme values
e If either Precision or Recall is very low, F1-Score will be low
e¢ Ensures both metrics are reasonably good

Example:

e Precision = 1.0, Recall = 0.1
e Arithmetic mean = (1.0 + 0.1) /2=0.55
e Harmonic mean (F1) = 2 x (1.0 x 0.1) / (1.0 + 0.1) = 0.18

The Fi-Score better reflects that the model is not performing well overall.



==============================
FILE: class-sixteen.pdf
PAGE: 23
==============================

When to Use F1-Score?
Use when:

¢ You need to balance Precision and Recall
e Classes are imbalanced
¢ Both False Positives and False Negatives matter

Examples:

e Information retrieval systems
e¢ Medical diagnosis where both types of errors are important

¢ When you need a single metric to compare models

Fi-Score is particularly useful when you cannot afford to ignore either Precision or Recall.



==============================
FILE: class-sixteen.pdf
PAGE: 24
==============================

Classification Metrics Summary

Metric Formula Focus Use When
Accuracy |(TP+T7N)/Total | Overall correctness | Balanced classes
Error Rate | 1 — Accuracy Overall errors Balanced classes
Precision | TP/(TP + FP) Avoid false alarms | FP is costly

Recall TP/(TP+ FN) Catch all positives | FN is costly
Fi-Score | 2x(PxR)/(P +R) | BalanceP&R Both FP & FN costly




==============================
FILE: class-sixteen.pdf
PAGE: 25
==============================

Regression Metrics Overview
For regression problems (predicting continuous values), we use different metrics:

¢ MAE: Mean Absolute Error

e MSE: Mean Squared Error

e RMSE: Root Mean Squared Error
e R*: Coefficient of Determination

All these metrics evaluate how close predictions are to actual values.



==============================
FILE: class-sixteen.pdf
PAGE: 26
==============================

6. Mean Absolute Error (MAE)
Definition

MAE is the average of the absolute differences between predicted and actual values.

Formula
L<= .
MAE = — yi — ¥i|
i=l

Where:

¢ n=number of observations
e y; = actual value

¢ 4; = predicted value



==============================
FILE: class-sixteen.pdf
PAGE: 27
==============================

MAE Calculation Example

Predict house prices (in $1000s):

Actual Price (y;) Predicted Price (4;) Error (y; — y;) Absolute Error |y; — 4;|

250 240 NK) 10
300 <¥A0) -20 20
180 175 5 5

400 390 10 10
220 230 -10 10

MAE = eer _ - -—u

Average error is $11,000 in either direction.



==============================
FILE: class-sixteen.pdf
PAGE: 28
==============================

7. Mean Squared Error (MSE)

Definition

MSE is the average of the squared differences between predicted and actual values.

Formula



==============================
FILE: class-sixteen.pdf
PAGE: 29
==============================

MSE Calculation Example

Using the same house price data:

Actual (y;) Predicted (9;) Error (y; — §;) Squared Error (y; — ;)”

250 240 10 100
300 <YA0) -20 400
180 175 ) 25
400 390 10 100
220 230 -10 100
MSE — — rr Z =“ — 145




==============================
FILE: class-sixteen.pdf
PAGE: 30
==============================

8. Root Mean Squared Error (RMSE)

Definition

RMSE is the square root of MSE, bringing the error metric back to the original units.
Formula

RMSE = v MSE =




==============================
FILE: class-sixteen.pdf
PAGE: 31
==============================

RMSE Calculation Example
Using the MSE from our house price example:

RMSE = VMSE = V145 ~ 12.04
The RMSE is approximately $12,040.

Interpretation: The model's predictions deviate from actual prices by about $12,000 on
average, with larger errors weighted more heavily.



==============================
FILE: class-sixteen.pdf
PAGE: 32
==============================

MAE vs. MSE vs. RMSE Comparison

Using our house price example:

e MAE = 11 (average absolute error)
e MSE = 145 (average squared error)
e RMSE = 12.04 (root average squared error)

Key Difference:

¢ MAE treats all errors equally: |10), |20), |5|, |10], |10}
e MSE/RMSE penalize large errors more: 100, 400, 25, 100, 100

The error of 20 has 4x the impact in MSE compared to error of 10.



==============================
FILE: class-sixteen.pdf
PAGE: 33
==============================

MAE vs. RMSE: Which to Use?

Aspect MAE RMSE

Interpretation Average absolute error | Root average squared error
Units Same as target Same as target

Outlier Sensitivity | Low High

Large Error Penalty | Linear Quadratic

Use When

Outliers are noise

Large errors are critical

Rule of thumb:

e If all errors are equally bad ~ Use MAE

e If large errors are worse > Use RMSE




==============================
FILE: class-sixteen.pdf
PAGE: 34
==============================

9. R? (R-Squared / Coefficient of Determination)

Definition

R? represents the proportion of variance in the dependent variable that is predictable from the
independent variable(s). It ranges from 0 to 1 (can be negative for poor models).

Formula

SSres

Ra —
SStot

Where:

© SSres = D531 (yi — i)” (Residual Sum of Squares)
© SSict = >>\_1 (yi — 9)? (Total Sum of Squares)

¢ y= mean of actual values



==============================
FILE: class-sixteen.pdf
PAGE: 35
==============================

R? Calculation Example

House prices: y = [250, 300, 180, 400, 220]

Mean: y =

250+300-+180+400+220
5

— 1350
5

= 27

250 | 240 100 400
300 | 320 400 900
180 | 175 25 8100
400 | 390 100 16900
220 | 230 100 2500




==============================
FILE: class-sixteen.pdf
PAGE: 36
==============================

R? Calculation (Continued)
SSres = 100 + 400 + 25 + 100 + 100 = 725
SStot = 400 + 900 + 8100 + 16900 + 2500 = 28800

725

R? =1-———
28800

= 1— 0.0252 = 0.9748

R? = 0.9748 or 97.48%

The model explains 97.48% of the variance in house prices.



==============================
FILE: class-sixteen.pdf
PAGE: 37
==============================

Interpreting R?

e R? = 1.0: Perfect predictions (all points on the line)
e R?= 0.9: 90% of variance explained (very good)

¢ R? = 0.7: 70% of variance explained (good)

e R?=0.5: 50% of variance explained (moderate)

e R?= 0.0: Model no better than predicting the mean
e R? < 0: Model worse than predicting the mean

Higher R? = Better model fit

But beware: High R? doesn't always mean a good model (could be overfitting).



==============================
FILE: class-sixteen.pdf
PAGE: 38
==============================

Regression Metrics Summary

Metric Formula Range _—Interpretation Use When

MAE + Solyi — §:| | [0,00) | Avg absolute error | Equal error weight

MSE |= >>(y:—%)?|[0,co) | Avg squared error | Penalize large errors

RMSE MSE [0,co) | Root avg sq.error | Interpretable + penalize

R? I= TSS (—oo, 1] | Variance explained | Model comparison

Lower is better for MAE, MSE, RMSE. Higher is better for R?.




==============================
FILE: class-sixteen.pdf
PAGE: 39
==============================

Choosing the Right Metric: Decision Guide
For Classification:

1. Are classes balanced? > Accuracy

2. Ils FP costly? > Precision

3. Is FN costly? > Recall

4. Are both FP & FN costly? ~ F1-Score

5. Need detailed analysis? + Confusion Matrix

For Regression:

1. Want simple interpretation? ~ MAE

2. Want to penalize large errors? ~ RMSE

3. Want to measure explained variance? > R?
4. For optimization? ~ MSE



==============================
FILE: class-sixteen.pdf
PAGE: 40
==============================

Real-World Examples
Medical Diagnosis (Cancer Detection):

e Metric: Recall (minimize False Negatives)
e Reason: Missing a cancer diagnosis is catastrophic

Credit Card Fraud Detection:

e Metric: F1-Score or Precision and Recall

e Reason: Balance catching fraud vs. blocking legitimate transactions
House Price Prediction:

e Metric: RMSE and R?
e Reason: Penalize large prediction errors, measure overall fit



==============================
FILE: class-sixteen.pdf
PAGE: 41
==============================

Common Mistakes to Avoid
1. Using Accuracy with imbalanced data
ec Can be misleading when one class dominates
2. Ignoring the cost of different errors
o FP and FN often have different impacts
3. Using MSE for interpretation
eo Squared units are hard to understand; use RMSE
4. Relying only on R?
ec Doesn't show prediction error magnitude

5. Not using Confusion Matrix
oc Always start with confusion matrix for classification



==============================
FILE: class-sixteen.pdf
PAGE: 42
==============================

Practice Problem 1: Classification

A medical test for a rare disease (5% prevalence) gives these results for 1000 patients:

Predicted Positive Predicted Negative

Actual Positive 45 5
Actual Negative 95 855
Calculate:
1. Accuracy
2. Precision
3. Recall

4. F1-Score



==============================
FILE: class-sixteen.pdf
PAGE: 43
==============================

Solution: Practice Problem 1

Given: TP=45, FN=5, FP=95, TN=855

1. Accuracy = 224855 — 200 _ 9 90 or 90%

1000 1000
Pr a ee
2. Precision = Boe =m 0.321 or 32.1%
3. Recall = => = = 0.90 or 90%

= 0.321x0.90 _ 0.289 _
4. F1-Score = 2 x ial Mee = 2X Foor — 0.473 or 47.3%



==============================
FILE: class-sixteen.pdf
PAGE: 44
==============================

Analysis: Practice Problem 1

e High Accuracy (90%): But misleading! Predicting everyone as negative gives 95% accuracy.
e Low Precision (32.1%): Many false alarms — only 1 in 3 positive predictions is correct.

e High Recall (90%): Good at catching actual positive cases.

¢ Moderate F1 (47.3%): Reflects the imbalance between precision and recall.

Conclusion: This test has too many false positives. Might need confirmation test for positive
results.



==============================
FILE: class-sixteen.pdf
PAGE: 45
==============================

Practice Problem 2: Regression

Predict student exam scores:

Actual Predicted

85 80
90 92
75 78
95 90
70 68

Calculate: MAE, MSE, RMSE, R?



==============================
FILE: class-sixteen.pdf
PAGE: 46
==============================

Solution: Practice Problem 2 (Part 1)

Mean actual: y =

8519057549570 _ 415 _ gg
H% YW-Ul Y-F
5
20) | 92 2 49
V5 | 73 i} 64
95 |90 5 25 144
70 | 68 2 4 169
Sum 17 67 430




==============================
FILE: class-sixteen.pdf
PAGE: 47
==============================

Solution: Practice Problem 2 (Part 2)
MAE = = = 3.4 points

MSE = © = 13.4 points?

RMSE = 13.4 ~ 3.66 points

R?=1— $7 = 1 — 0.156 = 0.844 or 84.4%

Interpretation: Predictions are off by ~3.4 points on average (MAE) or ~3.66 points with large
errors weighted more (RMSE). The model explains 84.4% of score variance.



==============================
FILE: class-nineteen.pdf
PAGE: 1
==============================

Machine Learning
K-Means Clustering

Unsupervised Learning



==============================
FILE: class-nineteen.pdf
PAGE: 2
==============================

Class Objectives
By the end of this class, students will be able to:

e Understand what K-Means clustering is

Explain when to use K-Means

Perform the K-Means algorithm step-by-step

Calculate distances and update centroids manually
e Interpret clustering results



==============================
FILE: class-nineteen.pdf
PAGE: 3
==============================

What Is K-Means?

K-Means is a centroid-based clustering algorithm that:

e¢ Automatically groups data into K clusters
¢ Works on unlabeled data (unsupervised)
e Uses distance measures to assign points to clusters

e Iteratively updates cluster centers (centroids) until convergence

Distance Measure: Euclidean Distance

d(x, a) = V(x — ay)? + (x2 — a2)?



==============================
FILE: class-nineteen.pdf
PAGE: 4
==============================

When to use it

Use K-Means when:

e You want grouping/segmentation (customers, documents, locations, behaviors)
e Features are numeric and “distance” is meaningful
e You expect clusters to be roughly compact (not weird shapes)

Avoid or be careful when:

e Clusters are non-spherical / varying density
e Many outliers exist
e Kis unknown and hard to pick



==============================
FILE: class-nineteen.pdf
PAGE: 5
==============================

K-Means Algorithm
Input: Dataset D, Number of clusters K

Algorithm Steps:

1. Initialize K centroids (random points or K-means++)

2. Assignment Step: Assign each point to the nearest centroid

3. Update Step: Recompute each centroid as the mean of assigned points

4. Repeat steps 2-3 until centroids/assignments stop changing (convergence)



==============================
FILE: class-nineteen.pdf
PAGE: 6
==============================

Numerical Example (K = 3)

Data Points:

Al 2/10
AW 2)5
A3 8) 4
Bi 5/8
B2 7s
B3 6/4
C1 al || 4
C2 419

Initial Centroids (given):

Ci= (2,10) C;=(5,8) C,—(1,2)



==============================
FILE: class-nineteen.pdf
PAGE: 7
==============================

Epoch 1 — Assignment Step

Distance table (C1=(2,10), C2=(5,8), C3=(1,2))

Point d(C1) d(C2) d(C3) Cluster

Al 0.00 /3.61 | 8.06 |1
WW 5.00 | 4.24 | 3.16
A3 8.49 |5.00 | 7.28
Bi 3.61 |0.00 | 7.21
B2 7.07 | 3.61 | 6.71
B3 7.21 |4.12 | 5.39
Cal 8.06 | 7.21 | 0.00
C2 2.24 |1.41 | 7.62

NO} @WOI NMI NIN] NM] &




==============================
FILE: class-nineteen.pdf
PAGE: 8
==============================

Updated Centroids:

¢ Cluster 1: {A1} + C; = (2,10)
* Cluster 2: {A3, B1, B2, B3, C2} > C2 = (6,6)
¢ Cluster 3: {A2, C1} > C3 = (1.5, 3.5)



==============================
FILE: class-nineteen.pdf
PAGE: 9
==============================

Epoch 2 — reassign using updated centroids

Updated Centroids:

Distance table (Epoch 2):

C; = (2,10),

C3 = (1.5, 3.5)

Point d(C1) d(C2) d(C3) New Cluster

Al 0.00 |5.66 |6.52 |1
LW 5.00 |4.12 |1.58 |3
A3 8.49 | 2.83 |6.52 |2
B1 3.61 | 2.24 |5.70 |2
B2 7.07 | 1.41 |5.70 |2
B3 7.21 |2.00 |4.53 |2
(Cal 8.06 |6.40 |1.58 |3
C2 2.24 |3.61 |6.04 | 1




==============================
FILE: class-nineteen.pdf
PAGE: 10
==============================

Update Centroids Again:
= COS) Ch= C5525) Cy = (14,35)



==============================
FILE: class-nineteen.pdf
PAGE: 11
==============================

Epoch 3 — reassign using updated centroids

Updated Centroids:

C1 = (3, 9.5),

Distance table (Epoch 3):

C2 = (6.5, 5.25),

C3 = (1.5, 3.5)

Point d(C1) d(C2) d(C3) New Cluster

Al 1.12 |6.54 |652 |1
W 4.61 |4.51 |/1.58 |3
A3 7.43 /1.95 |6.52 |2
B1 2.50 |3.13 [5.70 |1
B2 6.02 |0.56 |5.70 |2
B3 6.26 /1.35 | 4.53 |2
(Cal 7.76 |6.39 |1.58 |3
C2 1.12 |4.51 |6.04 |1




==============================
FILE: class-nineteen.pdf
PAGE: 12
==============================

Updated Centroids:
Ci = (Cr,D) Cy = (648s) (Ca = (1145, 85)



==============================
FILE: class-nineteen.pdf
PAGE: 13
==============================

Epoch 4 — reassign and check convergence

Updated Centroids:

C; = (3.67, 9),

Distance table (Epoch 4):

Cy = (7,4.33),

C3 = (1.5, 3.5)

Point d(C1) d(C2) d(C3) New Cluster

Al 1.94 |7.56 |6.52 |1
W 4.33 |5.04 |/1.58 |3
A3 6.62 |1.05 |6.52 |2
B1 1.67 |4.18 |5.70 |1
B2 5.21 |0.67 |5.70 |2
B3 5.52 /1.05 | 4.53 |2
(Cal 749 |6.44 |1.58 |3
C2 0.33 /5.55 | 6.04 |1




==============================
FILE: class-nineteen.pdf
PAGE: 14
==============================

Cluster assignments are unchanged ~ Convergence reached



==============================
FILE: class-seventeen.pdf
PAGE: 1
==============================

Support Vector Machine (SVM)



==============================
FILE: class-seventeen.pdf
PAGE: 2
==============================

What is SVM?

A Beginner's Introduction
Imagine you have:

¢ Red balls and blue balls scattered on a table
e You want to draw a line separating them

SVM does exactly this!

e It finds the best line (or boundary) to separate different groups
¢ The "best" line is the one with maximum distance from both groups
¢ This distance is called the margin



==============================
FILE: class-seventeen.pdf
PAGE: 3
==============================

Why Use SVM?
Key Advantages

Works great with clear separation

e Finds the optimal boundary between classes
Handles high-dimensional data

e Effective even when you have many features
Memory efficient

e Only uses a subset of training points (support vectors)



==============================
FILE: class-seventeen.pdf
PAGE: 4
==============================

Linear SVM
Concept

e SVM finds an optimal separating hyperplane

© In 2D:a line | In 3D: a plane | In higher dimensions: a hyperplane
¢ The hyperplane is defined by support vectors

°o These are the closest points from each class to the boundary
e Only a few boundary points decide the classifier

° Most data points don't affect the decision boundary



==============================
FILE: class-seventeen.pdf
PAGE: 5
==============================

Understanding Support Vectors
What Are They?

Support Vectors are:

e The data points closest to the decision boundary
e The critical points that define where the boundary goes
e Like anchors holding a rope (the boundary) in place

Why are they important?

¢ Moving a support vector changes the boundary
¢ Moving other points doesn't affect the boundary



==============================
FILE: class-seventeen.pdf
PAGE: 6
==============================

Linear SVM
Example
Positive labelled points
(3,1), (3,1), (631), (6,1)
Negative labelled points

(1,0), (On): (OS (—1,0)



==============================
FILE: class-seventeen.pdf
PAGE: 7
==============================

Visual: Linear SVM Dataset
Plotting the Points

positive
negative

Observation: Positive points are on the right, negative points on the left



==============================
FILE: class-seventeen.pdf
PAGE: 8
==============================

Linear SVM

Support Vector Selection
From the graph, we identify:

e Negative support vector (closest negative point)
5, = (1,0,1)
¢ Positive support vectors (closest positive points)

Note: Last component {i is added for bias term (intercept)



==============================
FILE: class-seventeen.pdf
PAGE: 9
==============================

Visual: Support Vectors & Boundary

Linear SVM Decision Line

[+] = Support Vector
© = Support Vector

Decision Boundary (y = x - 2)

Key: Only boxed points [e] are support vectors!



==============================
FILE: class-seventeen.pdf
PAGE: 10
==============================

Linear SVM

Generalization Equations
For each support vector, we create an equation:
a8) + 8 +028) + 85 +038) +85 = —l
185-81 + 284 + $y +.0389°53 =1
0185 - $4 +. 0285-85 +0383-83 =1

The dot product (-) measures similarity between vectors



==============================
FILE: class-seventeen.pdf
PAGE: 11
==============================

Step 1: Calculate Dot Products

Recall support vectors:
s, = (1,0,1), s2 = (3,1, 1), 53 = (3, -1, 1)

Compute all dot products:
sj-s;=17+0? +1? =2
8-8) = 1(3) + 0(1) +1(1) =4
$1 -° 83 = 1(3)+0(-1)+1(1) =4




==============================
FILE: class-seventeen.pdf
PAGE: 12
==============================

Step 2: More Dot Products
85° 8, = 3(1) +1(0) +1(1) =4
oe lee lo oll
8° 8, = 3(3) + 1(-1) +1(1) =9
83°; = 3(1) + (—1)(0) + 1(1) =4
#94 = 3(3) + (-1)() + 11) =9

8,°8,=37+(-1?+VP=11



==============================
FILE: class-seventeen.pdf
PAGE: 13
==============================

Step 3: Substitute into Equations

Equation 1:

2a, + 4a, + 4a3 = —1
Equation 2:

4a; + 1llaz+ 9a3 = 1
Equation 3:

4a, =F Jao =F llag = Il



==============================
FILE: class-seventeen.pdf
PAGE: 14
==============================

Step 4: Solve the System
Simplify Equation 1: Divide by 2
a, + 2a2g + 2a3 = —0.5
Subtract Equation 2 from Equation 3:

—2a2 + 2a3 = 0 An = a3




==============================
FILE: class-seventeen.pdf
PAGE: 15
==============================

Step 5: Final Substitution

Since a2 = a3, substitute into simplified Eq 1:
ay + 2a2g + 2a2 = —0.5
a1 +409 =-0.5 ...(A)
Substitute into Equation 2:
4a, +1lag+ 9a, =1
4a, + 20a, =1  ...(B)



==============================
FILE: class-seventeen.pdf
PAGE: 16
==============================

Step 6: Solve for a Values
From (A): a; = —0.5 — 4az
Substitute into (B):
4(—0.5 — daz) + 2002 = 1
—2 — 16a + 20a2 = 1
4ayg = 3 a2 = 0.75

Therefore: a3 = 0.75 and a; = —0.5 — 4(0.75) = —3.5



==============================
FILE: class-seventeen.pdf
PAGE: 17
==============================

Linear SVM

Solving a Values

After solving the equations:

a, = —-3.5, a2g=0.75, a3 =0.75



==============================
FILE: class-seventeen.pdf
PAGE: 18
==============================

Linear SVM

Hyperplane Calculation
We combine support vectors with their weights:
w= ne Q4S8;
Substituting our values:
W' = —3.5(1,0, 1) + 0.75(3, 1, 1) + 0.75(3, —1, 1)
Result:
W’' = (1,0, —2)



==============================
FILE: class-seventeen.pdf
PAGE: 19
==============================

Linear SVM
Final Equation
The decision boundary equation:
y=Wa+b
Where:

e W = (1,0) > weight vector (slope)
¢ b=2- bias term (intercept)

To classify the hyperplane:
b—a=0|b—2=0
b=alb=2

e If W = (1,0) -> Vertical Line
¢ If W = (0,1) -> Horizontal Line



==============================
FILE: class-seventeen.pdf
PAGE: 20
==============================

Visual: SVM Margin

positive
negative




==============================
FILE: class-seventeen.pdf
PAGE: 21
==============================

Practice Task: Solve YourOwn SVM
New Support Vectors
Given three support vectors:
S$; = (2,1,1), S2=(2,-1,1), S3 = (4,0,1)
Your Task:

1. Calculate all dot products S; - 5;

2. Set up the system of equations using:
© S$) -Sjai +S} -S3a2+ S}-S3a3 = —1
o S4- Sja1 + 95 -S3a2+ S5,- S303 =1
o $3-Sja,+S3-Sja2+ S3-S3a3 =1

3. Solve for a1, a2, a3




==============================
FILE: class-seventeen.pdf
PAGE: 22
==============================

Solution: Practice Task Results
Dot Products Calculation
All dot products:
S,-S,=2417+4+1°=6
Si - Sj = 2(2)+1(-1) +1(1) =4
S| - S3 = 2(4) +.1(0) +1(1) =9
Sey — 22) ea
S,-95=2?+(-17+1=6
S3-S3 = 2(4) + (-1)(0) + 1(1) = 9
53-9; = 4(2) + 0(1) +1(1) =9
S3-S5 = 4(2) + 0(-1) + 1(1) =9
$3-S,=¥404+7=17



==============================
FILE: class-seventeen.pdf
PAGE: 23
==============================

Solution: System of Equations
Substituted Equations

Equation 1:

6a, + 4a + 9a3 = —1
Equation 2:

4a, + 6az2 + 9a3 =1
Equation 3:

9a; + 9a2+17a3 = 1



==============================
FILE: class-seventeen.pdf
PAGE: 24
==============================

Solution: Final Alpha Values

Results

After solving the system:

ay= —2, a Dry a35— —0.5



==============================
FILE: Intro-to-RAG.pdf
PAGE: 1
==============================

Retrieval-Augmented Generation (RAG)

From Static Models to Knowledge-Aware Al Systems

Conceptual Overview
Machine Learning / Applied Al



==============================
FILE: Intro-to-RAG.pdf
PAGE: 2
==============================

Why Are We Talking About RAG?
Traditional ML / LLM Problems

¢ Models hallucinate
e Knowledge is static
e No awareness of your documents
¢ Cannot cite sources

e Unsafe for legal, medical, policy use

“Sounds confident” # “Correct”



==============================
FILE: Intro-to-RAG.pdf
PAGE: 3
==============================

Real-World Question

How do you build an Al system that:

e Answers only from trusted documents
¢ Does not make up facts
¢ Can be updated without retraining

Works for legal, policy, healthcare domains?



==============================
FILE: Intro-to-RAG.pdf
PAGE: 4
==============================

The Idea Behind RAG

RAG = Search first, then generate

Instead of asking an LLM to remember everything,
we let it:

1. Find relevant information

2. Answer using that information only



==============================
FILE: Intro-to-RAG.pdf
PAGE: 5
==============================

What Is RAG?
Definition
Retrieval-Augmented Generation (RAG) is an architecture that:

e Retrieves relevant documents from a knowledge base
e Injects them into the prompt

¢ Generates an answer grounded in retrieved data



==============================
FILE: Intro-to-RAG.pdf
PAGE: 6
==============================

High-Level RAG Architecture

User Question
4

Query Embedding
J

Vector Database

J

Top-K Relevant Chunks
4

Large Language Model
J

Grounded Answer




==============================
FILE: Intro-to-RAG.pdf
PAGE: 7
==============================

Key Components of a RAG System

Component Purpose

Documents Source of truth
Chunking Break large text
Embeddings Convert text > vectors

Vector Database | Similarity search

LLM Generate final answer




==============================
FILE: Intro-to-RAG.pdf
PAGE: 8
==============================

Role of Data in RAG
What Type of Data Works Best?

Policies

Y Laws & regulations
Manuals & handbooks
WY FAQs

Research documents

_ Live transactional data
- Rapidly changing numeric data
~~ Raw tables without structure



==============================
FILE: Intro-to-RAG.pdf
PAGE: 9
==============================

Why Chunking Is Necessary
LLMs and vector models:

¢ Cannot process very large documents

e Need small, meaningful pieces
Chunking helps:

e¢ Preserve context
e Improve retrieval accuracy
e Reduce hallucinations



==============================
FILE: Intro-to-RAG.pdf
PAGE: 10
==============================

What Are Embeddings?
Intuition (No Math)

e¢ Embeddings convert text into numbers
¢ Similar meaning > similar vectors
e Enables semantic search

Example:

e “Citizenship Act” = “Nationality Law”
e “Weather” + “Legal policy”



==============================
FILE: Intro-to-RAG.pdf
PAGE: 11
==============================

Why Vector Databases?

Traditional databases:

e Match exact keywords
Vector databases:

¢ Match meaning
Example:

e Query: “Who qualifies for citizenship?”

e Retrieved text:
“Eligibility criteria for nationality...”



==============================
FILE: Intro-to-RAG.pdf
PAGE: 12
==============================

Where the LLM Fits In

Important rule:

The LLM does NOT know your documents
It only:

e Reads retrieved chunks
¢ Combines them into natural language

e Follows strict instructions

LLM = Writer, not source of truth



==============================
FILE: Intro-to-RAG.pdf
PAGE: 13
==============================

Grounded (RAG) vs Ungrounded Al

Ungrounded LLM RAG System

Hallucinates Uses documents
No citations Source-based
Static Easily updated
Risky Auditable




==============================
FILE: Intro-to-RAG.pdf
PAGE: 14
==============================

Example Use Cases

e Legal assistant

e Policy Q&A system

e¢ University handbook bot

e Healthcare knowledge assistant

¢ Company internal documentation search



==============================
FILE: Intro-to-RAG.pdf
PAGE: 15
==============================

When NOT to Use RAG

If you need:

e Real-time calculations
¢ Numerical forecasting
e Image generation

¢ Transaction processing

RAG is for knowledge retrieval, not reasoning alone.



==============================
FILE: Intro-to-RAG.pdf
PAGE: 16
==============================

Key Takeaway
RAG is a knowledge-grounded Al system which can "act" like chatbots
It combines:

e ML

¢ Information Retrieval
¢ Databases

e APIs

¢ Prompt design



==============================
FILE: Intro-to-RAG.pdf
PAGE: 17
==============================

What’s Coming Next

e Data preparation
¢ Chunking strategies

Embeddings
e Vector databases

Full RAG implementation

FastAPI service



==============================
FILE: Intro-to-RAG.pdf
PAGE: 18
==============================

Next class:

Data & Chunking - where RAG succeeds or fails



==============================
FILE: 1. data-in-rag.pdf
PAGE: 1
==============================

Data for RAG Systems
What Works, What Fails, and Why

Retrieval-Augmented Generation (RAG)



==============================
FILE: 1. data-in-rag.pdf
PAGE: 2
==============================

Why Data Matters More Than the Model

In RAG systems:
The model is useless without good data

e¢ LLM does not “know” your documents
e Retrieval quality depends entirely on data quality
e Bad data — confident but wrong answers



==============================
FILE: 1. data-in-rag.pdf
PAGE: 3
==============================

What Is “Good Data” for RAG?
Good RAG data is:

e Text-heavy
¢ Knowledge-based
¢ Relatively stable

¢ Written for humans

What we need?
Information someone would normally read



==============================
FILE: 1. data-in-rag.pdf
PAGE: 4
==============================

Data Types That Work Best

Ideal Sources

Policies & regulations
Manuals & handbooks
e FAQs

Legal documents

Research papers

Medical guidelines

These contain explanations, definitions, and rules.



==============================
FILE: 1. data-in-rag.pdf
PAGE: 5
==============================

Example: Good RAG Data

University Handbook

e¢ Admission rules
¢ Grading policies

e Attendance requirements
Legal Policy Document

e Eligibility criteria
e Rights and responsibilities
e Procedures

Perfect Questions like:

“Who is eligible for X?”
“What happens if Y?”



==============================
FILE: 1. data-in-rag.pdf
PAGE: 6
==============================

What Data Is BAD for RAG
1. Tables Without Structure
e Raw tables

e Scanned tables

¢ CSV dumps with no explanation
Why?

¢ No semantic meaning
¢ Poor chunking

e Weak retrieval



==============================
FILE: 1. data-in-rag.pdf
PAGE: 7
==============================

Example: Bad Table Data

ID | Code | Value | Flag
12 | A32|0.87|1

LLM question:
“What does A32 mean?”

No explanation — no useful answer



==============================
FILE: 1. data-in-rag.pdf
PAGE: 8
==============================

What Data Is BAD for RAG

2. Constantly Changing Transactional Data

e Bank transactions
e Live sensor data

e Stock prices

e Attendance logs

Why?

¢ Needs real-time systems
¢ RAG is static knowledge retrieval

e Leads to outdated answers



==============================
FILE: 1. data-in-rag.pdf
PAGE: 9
==============================

Rule of Thumb

If data changes:

¢ every second
¢ every minute

e every hour

Don’t use RAG



==============================
FILE: 1. data-in-rag.pdf
PAGE: 10
==============================

Mixed Data: Be Careful

Some datasets contain:

e Text + tables

e¢ Explanations + numbers
Keep:

e¢ Explanatory text
Remove or summarize:

e Raw numeric dumps



==============================
FILE: 1. data-in-rag.pdf
PAGE: 11
==============================

The Role of Data Cleaning
Before RAG, documents often contain:

e Headers & footers
¢ Page numbers

¢ Repeated titles

e OCR artifacts

e Watermarks

These pollute embeddings.



==============================
FILE: 1. data-in-rag.pdf
PAGE: 12
==============================

Common Cleaning Issues
Example OCR Noise

GOVERNMENT OF NEPAL
MINISTRY OF HOME AFFAIRS

Vectors become meaningless
Retrieval quality drops



==============================
FILE: 1. data-in-rag.pdf
PAGE: 13
==============================

What Should Be Removed?

Remove or clean:

e Page numbers

¢ Repeated headers/footers
¢ Table of contents

e References (optional)

¢ Scanned artifacts

Goal:
Only meaningful content remains



==============================
FILE: 1. data-in-rag.pdf
PAGE: 14
==============================

What Should Be Preserved?

Keep:

¢ Headings

¢ Section titles

¢ Definitions

e Lists and clauses

¢ Paragraph structure

These improve:

e Chunking
e Retrieval

e Answer quality



==============================
FILE: 1. data-in-rag.pdf
PAGE: 15
==============================

Data Cleaning Pipeline (Conceptual)

Raw Document
L

Text Extraction
dt

Noise Removal
dt

Clean Text

L

Chunking

Cleaning happens before embeddings.



==============================
FILE: 1. data-in-rag.pdf
PAGE: 16
==============================

Why Cleaning Affects Retrieval
Embeddings treat all text as important.
If noise exists:

¢ Noise gets embedded
¢ Noise gets retrieved
e LLM uses noise as context

Garbage in > garbage out



==============================
FILE: 1. data-in-rag.pdf
PAGE: 17
==============================

Practical Guidelines
Before using any dataset for RAG, ask:

e Is this text meant to be read by humans?
¢ Does it explain something?
¢ Will this still be valid next month?

¢ Can a paragraph answer a question?

If yes > good RAG data.



==============================
FILE: 1. data-in-rag.pdf
PAGE: 18
==============================

Key Takeaways

¢ RAG is data-first, not model-first

e Good documents matter more than fancy LLMs

e¢ Cleaning improves retrieval more than tuning prompts
¢ Not all data belongs in a RAG system



==============================
FILE: 2. chunking.pdf
PAGE: 1
==============================

Chunking for RAG Systems
Why It Exists, How It Works, and How It Breaks Everything

Retrieval-Augmented Generation (RAG)



==============================
FILE: 2. chunking.pdf
PAGE: 2
==============================

Why Chunking?
In RAG systems:

Good chunking = good answers
Bad chunking = hallucinations

Chunking affects:

e¢ Retrieval accuracy
¢ Context relevance

e Answer quality



==============================
FILE: 2. chunking.pdf
PAGE: 3
==============================

What Is Chunking?

Chunking is the process of breaking large documents into small, meaningful pieces of text
Each chunk becomes:

¢ One embedding
e One retrievable unit



==============================
FILE: 2. chunking.pdf
PAGE: 4
==============================

Why Chunking Exists?

¢ Documents are too large
e LLM context window is limited

¢ Embeddings work best on small text

One big document — poor retrieval



==============================
FILE: 2. chunking.pdf
PAGE: 5
==============================

What Happens Without Chunking?

Imagine asking:
“What is the attendance policy?”
If the entire handbook is one chunk:

e Similarity is diluted
e Retrieval is inaccurate

e Wrong context is sent to LLM



==============================
FILE: 2. chunking.pdf
PAGE: 6
==============================

Chunking as a Search Problem
Think of chunking as:
“How can | split text so that one chunk answers one question?”
If a chunk can answer:
e exactly one type of question

That’s a good chunk.



==============================
FILE: 2. chunking.pdf
PAGE: 7
==============================

Chunk Size: Too Large
Example

Chunk = 3 pages of text
Problems:

¢ Contains many topics
e Embedding becomes “average”
¢ Retrieval is vague

e LLM gets irrelevant context



==============================
FILE: 2. chunking.pdf
PAGE: 8
==============================

Chunk Size: Too Small

Example
Chunk = 1 sentence
Problems:

e Loses context
¢ Definitions split from explanations
e Multiple chunks needed to answer one question



==============================
FILE: 2. chunking.pdf
PAGE: 9
==============================

Chunk Size: Just Right
Rule of Thumb

e 300-1000 characters

e Or 150-300 words

¢ Keep related ideas together

Goal:
One chunk = one concept



==============================
FILE: 2. chunking.pdf
PAGE: 10
==============================

Why Chunk Overlap Exists
Problem Without Overlap

If asentence is split:

¢ Half goes into chunk A
¢ Half into chunk B

Meaning is lost.



==============================
FILE: 2. chunking.pdf
PAGE: 11
==============================

Chunk Overlap Explained
Overlap means:

¢ Repeating some text between chunks
Example:

e Chunk 1: sentences 1-10
e Chunk 2: sentences 8-18

This preserves context.



==============================
FILE: 2. chunking.pdf
PAGE: 12
==============================

Typical Overlap Values

Chunk Size Overlap

300 chars | 50-80
600 chars | 100-150
1000 chars | 150-200

No overlap — fragile RAG.



==============================
FILE: 2. chunking.pdf
PAGE: 13
==============================

Heading-Based Chunking
What It Is

e Split text by headings
e Keep sections together

Example:
Attendance Policy

(text...)



==============================
FILE: 2. chunking.pdf
PAGE: 14
==============================

Why Heading-Based Chunking Is Powerful

¢ Preserves structure
¢ Keeps definitions + rules together
e Matches how humans read documents

Perfect for:

e Policies
e Manuals
¢ Handbooks



==============================
FILE: 2. chunking.pdf
PAGE: 15
==============================

Sentence-Based Chunking
What It Is

e Split text sentence by sentence
e¢ Group sentences until size limit

Good for:

e Plain articles
e Blogs
e Unstructured text



==============================
FILE: 2. chunking.pdf
PAGE: 16
==============================

Heading vs Sentence-Based Chunking

Heading-Based Sentence-Based

Structured Flexible

Clean retrieval More generic

Best for policies Best for articles

Requires headings | No headings needed




==============================
FILE: 2. chunking.pdf
PAGE: 17
==============================

What Happens When Chunking Is Bad

e¢ Answer mentions wrong section
e LLM mixes topics
e Retrieval returns unrelated chunks

e Hallucinations increase



==============================
FILE: 2. chunking.pdf
PAGE: 18
==============================

Real Example of Bad Chunking
Chunk contains:

e Attendance rules
¢ Grading system
¢ Exam policy

Question:
“What happens if | miss classes?”

LLM answer:
Mentions grading and exams



==============================
FILE: 2. chunking.pdf
PAGE: 19
==============================

Chunking Is NOT One-Size-Fits-All

Chunking depends on:

e Document type
e Language
¢ Question style

¢ Domain (legal, medical, academic)

Always test and adjust.



==============================
FILE: 2. chunking.pdf
PAGE: 20
==============================

How to Know Chunking Is Working
Ask:

e¢ Are retrieved chunks clearly relevant?
¢ Can one chunk answer the question?

e Do answers cite correct sections?

If yes > chunking is good.



==============================
FILE: 2. chunking.pdf
PAGE: 21
==============================

Key Takeaways

e¢ Chunking is the foundation of RAG

¢ Chunk size balances context and focus
¢ Overlap preserves meaning

¢ Headings make chunking smarter

e Bad chunking breaks everything



==============================
FILE: 2. chunking.pdf
PAGE: 22
==============================

What’s Next
Embeddings

¢ What they represent
e Why similarity works
e¢ How chunking affects embeddings



==============================
FILE: 3. embeddings.pdf
PAGE: 1
==============================

Embeddings

From Text to Meaningful Numbers



==============================
FILE: 3. embeddings.pdf
PAGE: 2
==============================

Recap: Why Chunking Matters
Before embeddings, we split text into chunks because:

¢ Models have token limits
¢ Smaller chunks > more focused meaning

e Better retrieval accuracy in search & RAG systems

Now let's see how chunks become numbers



==============================
FILE: 3. embeddings.pdf
PAGE: 3
==============================

What Is an Embedding?

An embedding is a numeric representation of meaning

e Text > Vector (list of numbers)
¢ Similar meanings > Similar vectors

¢ Different meanings — Distant vectors

Computers don't understand words
They understand numbers



==============================
FILE: 3. embeddings.pdf
PAGE: 4
==============================

Example (Conceptual)

Text chunks:

:
.
tm Car needs fuel"

Embeddings (simplified):

Dog > [@.12, 0.88, 0.31]

Cat > [@.18, @.85, 0.29]
Car + [0.91, @.82, 0.77]

Dog & Cat are close
Car is far away



==============================
FILE: 3. embeddings.pdf
PAGE: 5
==============================

What Embeddings Actually Represent
Each number in an embedding represents abstract features, like:

e Topic

¢ Context

e Intent

e Semantic meaning

These are NOT human-readable
You cannot say:

“Dimension 57 = happiness”

They work only in mathematical space



==============================
FILE: 3. embeddings.pdf
PAGE: 6
==============================

Think of Embeddings as Coordinates
Imagine a meaning space:

e Every sentence = a point
e Similar sentences = nearby points

¢ Different topics = far apart
Like a map of meaning

"Machine Learning basics"
"Intro to Al" > close
"Cooking pasta" — far



==============================
FILE: 3. embeddings.pdf
PAGE: 7
==============================

What Happens After Embedding?
Typical pipeline:

1. Chunk text

2. Convert chunks to embeddings

3. Store embeddings in a vector database
4. Embed user query

5. Find most similar vectors

6. Retrieve matching chunks

This is the core of semantic search



==============================
FILE: 3. embeddings.pdf
PAGE: 8
==============================

Why Not Compare Words Directly?
Keyword matching fails:

e "car" # "automobile"

e "Al" # "Artificial Intelligence"
Embeddings capture meaning, not spelling
That’s why:

“How to train a model” = “Steps for ML training”



==============================
FILE: 3. embeddings.pdf
PAGE: 9
==============================

Measuring Similarity Between Embeddings
Once we have vectors, we need to answer:

How similar are these two meanings?

Common options:

e Euclidean distance
¢ Dot product
¢ Cosine similarity (most common)



==============================
FILE: 3. embeddings.pdf
PAGE: 10
==============================

Why Cosine Similarity Works
Cosine similarity measures:

Angle between vectors, not distance
Why this matters:

e¢ Length of vector + importance
¢ Direction = meaning

Same direction > similar meaning
Opposite direction > very different meaning



==============================
FILE: 3. embeddings.pdf
PAGE: 11
==============================

Visual Intuition

¢ Two vectors pointing the same way — cosine = 1
e At 90° > cosine = 0

¢ Opposite direction — cosine = -1
In practice:

e Values closer to 1 = very similar
e Near 0 = unrelated



==============================
FILE: 3. embeddings.pdf
PAGE: 12
==============================

Cosine Similarity (Simple Formula)
cosine(A, B) = (A ~~ B) / (|A| x [BI)

Libraries will handle this for you

What matters:
Angle = meaning similarity



==============================
FILE: 3. embeddings.pdf
PAGE: 13
==============================

Embedding Dimensions Explained

Embedding dimension =
How many numbers represent one chunk

Common sizes:

° 384
¢ 768
e 1536

Each number adds more expressive power



==============================
FILE: 3. embeddings.pdf
PAGE: 14
==============================

Think of Dimensions Like Detail Level

Analogy: Image resolution

Dimensions Like Meaning

384 Low-res image | Fast, less detail
768 HD image Balanced
1536 Ultra-HD High detail, slower




==============================
FILE: 3. embeddings.pdf
PAGE: 15
==============================

384 Dimensions

Very fast
Low memory usage
Less semantic nuance

Good for:

¢ Small projects

e Fast search

e Prototypes

¢ Mobile / edge systems



==============================
FILE: 3. embeddings.pdf
PAGE: 16
==============================

768 Dimensions

Balanced accuracy
Moderate speed
Most common choice

Good for:

e Production RAG systems
e Knowledge bases
¢ Educational projects



==============================
FILE: 3. embeddings.pdf
PAGE: 17
==============================

1536 Dimensions

High semantic accuracy
Slower search
More storage required

Good for:

e Legal documents
¢ Medical data
e Research papers

e Large enterprise systems



==============================
FILE: 3. embeddings.pdf
PAGE: 18
==============================

Why the numbers

Model Embedding Dimension

MiniLM 384
BERT-base 768

(@}ey-JayANe text-embedding-3-small Mmm sio)
(@}ey-Vay-Ne text-embedding-3-large [aCLeyy4

Word2Vec (classic) 100-300
GloVe 50, 100, 200, 300
Custom research models Any N

Among these, 384, 768 and 1536 are widely used



==============================
FILE: 3. embeddings.pdf
PAGE: 19
==============================

Trade-Off: Speed vs Accuracy

Factor Low Dimensions High Dimensions
Speed Fast Slower

Memory | Low High

Accuracy | Medium High

Cost Low Higher

There is no best size
Only best for your use case




==============================
FILE: 3. embeddings.pdf
PAGE: 20
==============================

Practical Rule of Thumb

e Start with 768

¢ Optimize later

e Increase only if:
°o Results feel vague
© Meanings are very subtle
© Domain is complex

Bigger # Always better



==============================
FILE: 3. embeddings.pdf
PAGE: 21
==============================

Chunking + Embeddings Together
Why both matter:

e¢ Chunking controls context size

¢ Embeddings control semantic understanding

Bad chunking — bad embeddings
Good chunking — meaningful embeddings



==============================
FILE: 3. embeddings.pdf
PAGE: 22
==============================

Common Beginner Mistakes

e Using very large chunks

e Assuming embeddings are interpretable

¢ Choosing max dimensions blindly

e¢ Using keyword search instead of semantic search



==============================
FILE: 3. embeddings.pdf
PAGE: 23
==============================

Mental Model to Remember

¢ Chunking = What to read
e Embeddings = How meaning is stored
¢ Cosine similarity = How meaning is compared

¢ Dimensions = How detailed meaning is



==============================
FILE: 3. embeddings.pdf
PAGE: 24
==============================

Upcoming Topics

e¢ Vector databases

e Approximate nearest neighbor (ANN)
e RAG architecture

¢ Embedding models in practice



==============================
FILE: 3. embeddings.pdf
PAGE: 25
==============================

Key Takeaways

Embeddings turn text into numbers

¢ Similar meaning > similar vectors

¢ Cosine similarity compares direction

¢ Dimension size affects speed & accuracy

e Choose based on use case, not hype



==============================
FILE: 4. vectordb.pdf
PAGE: 1
==============================

Vector Databases (for RAG)



==============================
FILE: 4. vectordb.pdf
PAGE: 2
==============================

The core problem RAG solves

When a user asks a question, you want to fetch the most relevant chunks from your knowledge
base.

Two big requirements:

¢ Semantic match (meaning, not exact keywords)
e Fast retrieval at scale

Vector DBs are built for this.



==============================
FILE: 4. vectordb.pdf
PAGE: 3
==============================

What is a vector (embedding)?
A chunk of text — converted into a list of numbers:

Om embedding = [@.12, -@.03, 0.88, ...

e¢ Each chunk becomes a point in a high-dimensional space

¢ Similar meaning — points are closer

Retrieval becomes: find nearest vectors to the query vector.



==============================
FILE: 4. vectordb.pdf
PAGE: 4
==============================

What is a Vector Database?

A database optimized to store and query:

¢ Vectors (embeddings)
e IDs
e Metadata (JSON fields)

¢ Sometimes the raw text too
Main feature:

e ANN search (Approximate Nearest Neighbors)
e Returns top-K most similar vectors quickly



==============================
FILE: 4. vectordb.pdf
PAGE: 5
==============================

What vector search returns
Given:

* query embedding
e stored embeddings

Vector search returns:

° matches

e each with a similarity score (cosine / dot / L2)
e plus associated metadata + chunk text



==============================
FILE: 4. vectordb.pdf
PAGE: 6
==============================

Why “Approximate” Nearest Neighbor?
Exact nearest neighbor search is expensive at scale.
Vector DBs use indexing structures:

e¢ HNSW (graph-based)
e IVF (cluster-based)
e PQ (compression)

Trade-off:

e Slight loss in recall
¢ Huge gain in speed + cost



==============================
FILE: 4. vectordb.pdf
PAGE: 7
==============================

Why not SQL?
SQL is great for:
e Exact matches
e¢ Range queries
e Joins
e Aggregations
But SQL is NOT designed for:

e Finding the 20 most semantically similar chunks to this sentence
e Efficient ANN indexes
¢ Similarity search over high-dimensional float arrays at scale



==============================
FILE: 4. vectordb.pdf
PAGE: 8
==============================

What about PostgreSQL's pgvector

True. You can do vector search in SQL using extensions (like pgvector).
When it works well:

¢ Small/medium datasets
¢ You want strong relational joins + vectors

e You want fewer moving parts
When dedicated vector DB wins:

e Very large scale

¢ Heavy ANN tuning

¢ High throughput retrieval

e¢ Hybrid search + filtering performance



==============================
FILE: 4. vectordb.pdf
PAGE: 9
==============================

What a typical RAG record looks like
Each chunk you store usually needs:

¢ ER]: unique chunk ID

¢ QASEe: embedding array

¢ RS: the actual chunk content

¢ GREE: fields for filtering + traceability



==============================
FILE: 4. vectordb.pdf
PAGE: 10
==============================

Example metadata fields:

° (pdf, web, doc)
,

Ul page

,

,
° (multi-user systems)



==============================
FILE: 4. vectordb.pdf
PAGE: 11
==============================

Metadata storage (why it matters)
Vector similarity alone is not enough.
Metadata helps you:

e Filter to correct scope (e.g., a single course / client)

e¢ Maintain citations / provenance (page numbers, URLs)
¢ Debug retrieval (“why did this chunk appear?”)

e Support access control (RBAC, per-user documents)

In RAG, metadata is not optional.



==============================
FILE: 4. vectordb.pdf
PAGE: 12
==============================

Filtering + retrieval (the common pattern)
You rarely want:

e “Search across everything”
You usually want:

e “Search within the right subset”

Examples:

* Only this user's docs:
¢ Only policies:

¢ Only recent docs:
° Only relevant tags:

So retrieval becomes:

vector search + metadata filter



==============================
FILE: 4. vectordb.pdf
PAGE: 13
==============================

Example: retrieval request (conceptual)
Inputs:

e query text
e filter conditions

e top_k
Process:

1. Embed the query > [J

2. ANN search in vector index

3. Apply metadata filters (pre or post, depends on DB)
4. Return top_k chunks + metadata



==============================
FILE: 4. vectordb.pdf
PAGE: 14
==============================

Pre-filter vs Post-filter
Two ways vector DBs handle filters:
Pre-filter (preferred)

¢ Apply metadata constraints during ANN search

e Faster and more accurate for constrained scopes
Post-filter

e Search broadly first, then filter results
e Can miss good matches if filtered set is small

For RAG with strict scopes (tenant/user), pre-filter matters.



==============================
FILE: 4. vectordb.pdf
PAGE: 15
==============================

The RAG pipeline (where vector DB fits)

1. Ingest

© load documents

° clean text

© chunk
2. Embed

© generate embeddings for each chunk
3. Store

© vector + text + metadata in vector DB
4. Retrieve

© embed query

© search + filter
5. Generate

© send retrieved chunks to LLM with prompt + citations



==============================
FILE: 4. vectordb.pdf
PAGE: 16
==============================

Chunking (required for good RAG)
You do NOT embed whole PDFs.
You embed chunks:

e 200-800 tokens per chunk (common range)

e with overlap (e.g., 10-20%) to avoid cutting context
Chunking must preserve:

e section headings
¢ page info

e document identity
e ordering info

Bad chunking = bad retrieval.



==============================
FILE: 4. vectordb.pdf
PAGE: 17
==============================

What “good retrieval” means
A good retriever gets:

e relevant chunks (high precision)

¢ complete coverage (high recall)

e from the right scope (filters)

¢ with traceable sources (metadata)

RAG quality is often bottlenecked by retrieval, not the LLM.



==============================
FILE: 4. vectordb.pdf
PAGE: 18
==============================

Similarity metrics (quick)
Common options:

¢ Cosine similarity: compares direction (common for normalized embeddings)
e¢ Dot product: similar to cosine if vectors are normalized
e L2 distance: Euclidean distance

Most modern embedding workflows:

¢ normalize vectors
e use cosine or dot



==============================
FILE: 4. vectordb.pdf
PAGE: 19
==============================

Hybrid search (very important for RAG)
Vector search is semantic, but sometimes you need exact terms:

¢ names, IDs, error codes
¢ legal section numbers

e version strings
Hybrid search = combine:

¢ keyword search (BM25)
¢ vector search

Benefits:

e better for “needle” queries
e better for rare proper nouns
e better grounding for technical/legal RAG



==============================
FILE: 4. vectordb.pdf
PAGE: 20
==============================

Re-ranking (often necessary)
ANN gives you candidates, not perfect ordering.

Common pattern:

ME CcSa@ (Mem top_k = 50

2. Re-rank with a stronger model (cross-encoder / LLM rerank)
3. Keep top [aiminetemerea:)

This usually boosts answer quality a lot.



==============================
FILE: 4. vectordb.pdf
PAGE: 21
==============================

Storing text: in vector DB or elsewhere?
Two approaches:
Store text in vector DB

e simpler system
¢ easy retrieval

Store text in object store / SQL

¢ vector DB stores only IDs + metadata
e fetch text by ID after retrieval

Pick based on:

e scale
e cost

¢ system complexity



==============================
FILE: 4. vectordb.pdf
PAGE: 22
==============================

Multi-tenant RAG (must-have in products)
If many users upload documents:

Always store:

,
° (optional)

e access rules
And always filter retrieval by tenant/user scope.

If you don’t, you leak data across users.



==============================
FILE: 4. vectordb.pdf
PAGE: 23
==============================

Versioning + updates
Real systems have evolving docs.

Plan for:

:

e “soft delete” old chunks

¢ re-embedding on model change
¢ rebuild indexes when needed

Also store:

¢ embedding model name used (critical for migrations)



==============================
FILE: 4. vectordb.pdf
PAGE: 24
==============================

What to log for debugging RAG
At query time, log:

e query text

e filters applied

e retrieved chunk IDs + scores
¢ doc/page/section metadata

e final prompt context
This is how you diagnose:

e wrong chunks
e missing chunks
¢ over-filtering

¢ noisy embeddings



==============================
FILE: 4. vectordb.pdf
PAGE: 25
==============================

Common failure modes

e No metadata — cannot filter properly

¢ Wrong chunk size > either too vague or too fragmented
e No hybrid > fails on exact identifiers

e Norerank — weak ordering

¢ No scope filters  cross-user leakage

¢ Poor ingestion > garbage in, garbage out



==============================
FILE: 4. vectordb.pdf
PAGE: 26
==============================

Minimal “RAG-ready” vector DB checklist
You should be able to do:

e¢ Upsert vectors with metadata

e ANN search with top_k

e Metadata filtering (ideally pre-filter)
¢ Delete / soft-delete by doc_id

¢ Multi-tenant isolation

e Optional: hybrid search + rerank support



==============================
FILE: 4. vectordb.pdf
PAGE: 27
==============================

Summary

e Vector DB = storage + fast similarity search for embeddings
e¢ SQL is not optimized for high-dimensional ANN similarity search
e Metadata is required for filtering, traceability, and access control
¢ RAG quality improves with:

° chunking + overlap

© hybrid search

° reranking

© dedup/diversity

© good logging



==============================
FILE: 5. llm.pdf
PAGE: 1
==============================

LLM Foundations & RAG Architecture

From Text Generation
to
Retrieval-Augmented Generation (RAG)



==============================
FILE: 5. llm.pdf
PAGE: 2
==============================

Part 1
What LLMs Actually Do



==============================
FILE: 5. llm.pdf
PAGE: 3
==============================

What is a Large Language Model?
An LLM is:
A probabilistic next-token prediction engine
It does not:

e Think
e Reason like humans

¢ Know facts

It predicts the most likely next word.



==============================
FILE: 5. llm.pdf
PAGE: 4
==============================

How LLMs Generate Text

Input:

Machine learning is

Model predicts:

¢ powerful

e transforming
ea

e the

Each word has a probability.

The highest probability token is chosen (depending on decoding settings).



==============================
FILE: 5. llm.pdf
PAGE: 5
==============================

LLM = Pattern Recognition at Scale

During training:

e Trained on massive text corpora
e Learns statistical patterns
e Learns structure of language

e Learns relationships between words

It compresses language patterns into billions of parameters.



==============================
FILE: 5. llm.pdf
PAGE: 6
==============================

Important Reality

LLMs do not store knowledge like databases.
They store:

e Statistical relationships
e Semantic associations

e Pattern likelihoods

They generate answers.
They do not retrieve facts.



==============================
FILE: 5. llm.pdf
PAGE: 7
==============================

Tokenization

Before text goes into an LLM:
Text - Tokens ~ Numbers

Example:

Machine learning is powerful

Might become:

[1234, 5821, 98, 4411]



==============================
FILE: 5. llm.pdf
PAGE: 8
==============================

A token is not always a word.
It can be:

e Part of a word
e A full word
¢ Punctuation



==============================
FILE: 5. llm.pdf
PAGE: 9
==============================

Why Tokenization Matters

Because:

e Cost depends on number of tokens
e Context window depends on tokens
e¢ Truncation happens at token level

¢ Prompt engineering depends on token limits

Tokens are the real currency.



==============================
FILE: 5. llm.pdf
PAGE: 10
==============================

Context Window

The context window is:
The maximum number of tokens the model can see at once
Includes:

e System prompt
e User query
¢ Retrieved documents

¢ Model response

If exceeded:
Old tokens get truncated.



==============================
FILE: 5. llm.pdf
PAGE: 11
==============================

Why Context Window Is Critical for RAG

If you retrieve too many chunks:

e You exceed context window
¢ Important info gets cut

e Answers degrade

Retrieval must be controlled.



==============================
FILE: 5. llm.pdf
PAGE: 12
==============================

Hallucination

Hallucination = Model generates confident but incorrect information.
Why it happens:

¢ Model predicts probable text
e It fills gaps statistically
e It does not verify truth

Example:
Model invents citations.
Model fabricates facts.



==============================
FILE: 5. llm.pdf
PAGE: 13
==============================

Why LLM Alone Is Not Enough

Problems:

e No access to private data

¢ Knowledge cutoff

¢ Hallucination risk

¢ Cannot search databases

¢ Cannot access live information

Solution?



==============================
FILE: 5. llm.pdf
PAGE: 14
==============================

Why RAG Exists

RAG = Retrieval-Augmented Generation
Instead of asking LLM:

"Answer from your memory"

We say:

"Answer using these documents"

We inject retrieved knowledge into the prompt.



==============================
FILE: 5. llm.pdf
PAGE: 15
==============================

RAG Architecture Deep Dive



==============================
FILE: 5. llm.pdf
PAGE: 16
==============================

RAG Full Pipeline

User Query

— Embed query

— Search vector DB

— Retrieve top-k chunks

— Inject into prompt

— LLM generates grounded answer



==============================
FILE: 5. llm.pdf
PAGE: 17
==============================

Step 1: Query Embedding

User question:

What are admission requirements?

We convert it into a vector.
Then compare it against stored document vectors.

Similarity search begins.



==============================
FILE: 5. llm.pdf
PAGE: 18
==============================

Step 2: Top-k Selection

We retrieve:

Top 3
Top 5
Top 10

Most similar chunks.
Trade-off:

¢ Too small > Missing context

¢ Too large ~ Noise + token overflow

Choosing k is critical.



==============================
FILE: 5. llm.pdf
PAGE: 19
==============================

Step 3: Metadata Filtering

Vector similarity alone is not enough.
We can filter by:

e Document type
e Year

¢ Department

¢ Category

e Author

e Language

Example:

Only search in:
"Policy documents from 2024"

Hybrid filtering improves precision.



==============================
FILE: 5. llm.pdf
PAGE: 20
==============================

Step 4: Prompt Construction

We build a structured prompt:

System:
"You are an assistant. Use only provided context."

Context:

[Chunk 1]
[Chunk 2]
[Chunk 3]

User:
"What are admission requirements?"

This is grounding.



==============================
FILE: 5. llm.pdf
PAGE: 21
==============================

Step 5: Query Rewriting
Sometimes user queries are vague.

Example:
"Tell me about fees"

Rewrite to:
"What are the tuition fees for undergraduate programs in 2025?"

Rewriting improves retrieval quality.
Methods:

e LLM-based rewriting
e¢ Rule-based expansion

e Keyword expansion



==============================
FILE: 5. llm.pdf
PAGE: 22
==============================

Step 6: Re-ranking
After retrieving top-k:

We can:

e Re-score using a cross-encoder
e Use LLM to re-rank
¢ Combine keyword + vector scoring

Re-ranking improves precision before generation.



==============================
FILE: 5. llm.pdf
PAGE: 23
==============================

User
4
Query Processing

L
Embedding

L

Vector Search

L

Filtering

4

Re-ranking

4

Prompt Construction
L

LLM

L

Final Answer

Full RAG System Architecture




==============================
FILE: 5. llm.pdf
PAGE: 24
==============================

Common RAG Failure Points

Poor chunking

Too large chunk size

Bad top-k selection

No metadata filtering
Context window overflow
Weak prompt structure

High temperature



==============================
FILE: 5. llm.pdf
PAGE: 25
==============================

Key Insight

LLM = Generator
Vector DB = Retriever

RAG = Controlled Knowledge Injection



==============================
FILE: 5. llm.pdf
PAGE: 26
==============================

Example Scenario

Data Source:
College Handbook (PDF > Chunked ~ Embedded > Stored in Vector DB)



==============================
FILE: 5. llm.pdf
PAGE: 27
==============================

Step 1 - User Asks a Vague Question

User Query:
"Tell me about fees"
Problem:

¢ Which program?

¢ Domestic or international?

e Undergraduate or postgraduate?
¢ Tuition or other fees?

e Which year?

This is vague.



==============================
FILE: 5. llm.pdf
PAGE: 28
==============================

Step 2 - Query Rewriting (LLM Comes Into Play)

We use the LLM to clarify intent before retrieval.
System Prompt:

You are a query rewriting assistant.
Your task is to rewrite vague or ambiguous student questions into
clear, specific, and searchable academic queries.
Rules:
Default to the current academic year unless specified.

Default to domestic students unless specified.
Assume undergraduate level unless stated otherwise.
Do NOT answer the question.

Output only the rewritten query.

Do not add explanations.

User:
"Tell me about fees"



==============================
FILE: 5. llm.pdf
PAGE: 29
==============================

LLM Output (Rewritten Query)

"What are the tuition fees for undergraduate programs for domestic students in 2025?"
Now the query is structured and specific.

This improves retrieval quality.



==============================
FILE: 5. llm.pdf
PAGE: 30
==============================

Step 3 - Embedding & Vector Search

Rewritten Query ~ Embedding — Vector DB search
We retrieve Top-3 most similar chunks.
Example retrieved chunks:

Chunk 1:
"Undergraduate domestic tuition fees for 2025 are $8,500 per year..."

Chunk 2:
"Additional student services fee: $450 per semester..."

Chunk 3:
"International undergraduate tuition fees for 2025 are $14,000..."



==============================
FILE: 5. llm.pdf
PAGE: 31
==============================

Step 4 - Metadata Filtering

We filter:

e Program Level = Undergraduate
e Student Type = Domestic
e Year = 2025

Chunk 3 is removed (international).

Now only relevant chunks remain.



==============================
FILE: 5. llm.pdf
PAGE: 32
==============================

Step 5 - Final Answer Generation

Now the LLM generates the final answer.

Final System Prompt

"You are a university assistant.

Answer ONLY using the provided context.

If the answer is not in the context, say:

‘The information is not available in the handbook.’
Do not add external knowledge.

Be concise and structured."



==============================
FILE: 5. llm.pdf
PAGE: 33
==============================

Prompt Sent to LLM

System:
(above system prompt)

Context:
"Undergraduate domestic tuition fees for 2025 are $8,500 per year.
Additional student services fee: $450 per semester."

User:
"What are the tuition fees for undergraduate programs for domestic students in 2025?"



==============================
FILE: 5. llm.pdf
PAGE: 34
==============================

LLM Final Output

"For 2025, undergraduate tuition fees for domestic students are $8,500 per year.
In addition, a student services fee of $450 per semester applies."



==============================
FILE: 5. llm.pdf
PAGE: 35
==============================

What Just Happened?

1. User asked vague question.

2. LLM rewrote the query.

3. Query embedded.

4. Vector search performed.

5. Metadata filtering applied.

6. Context injected into prompt.

7. LLM generated grounded answer.



==============================
FILE: 5. llm.pdf
PAGE: 36
==============================

Where LLM Was Used
1. Query Rewriting
2. Final Answer Generation

Vector DB handled retrieval.
LLM handled language + reasoning.



==============================
FILE: 5. llm.pdf
PAGE: 37
==============================

Why This Matters

Without RAG:
LLM might hallucinate outdated fees.

With RAG:
Answer is grounded in handbook data.

This is controlled generation.

